{"id":"s-7ujm","uuid":"1771dedd-68b4-4ec8-98e8-fe8a77df8a9d","title":"Payment Token Service","file_path":"specs/payment_token_service.md","content":"## Overview\nMicroservice responsible for tokenizing payment data from POS hardware. This service handles device-based decryption, re-encryption with rotating keys, token storage, and secure token retrieval. **PCI compliance boundary** - isolated from other services.\n\n## Service Boundaries\n- **Owns**: Payment data encryption/decryption, BDK management, token generation, token-to-payment-data mapping, internal decryption for workers\n- **Does NOT own**: Authorization logic, payment processing, merchant configuration\n- **Isolation**: Separate database, separate deployment, minimal API surface\n\n## Encryption Architecture\n\n### Key Management\n- **BDK (Base Derivation Key)**: Master key stored in AWS KMS, never leaves the service\n- **Device Tokens**: Per-device identifiers provided by POS hardware\n- **Derived Keys**: Generated per-request using KDF(BDK, device_token)\n- **Service Rotating Keys**: Used to re-encrypt tokens, rotated regularly (e.g., every 90 days)\n\n### Encryption Flow\n\n**Token Creation:**\n```\nPOS Device:\n  1. Generates device_token (unique per device)\n  2. Derives encryption key: device_key = KDF(BDK, device_token)\n  3. Encrypts payment data: encrypted = AES-GCM(payment_data, device_key)\n  4. Sends: encrypted_payment_data + device_token\n\nPayment Token Service:\n  1. Retrieves BDK from AWS KMS\n  2. Derives same key: device_key = KDF(BDK, device_token)\n  3. Decrypts: payment_data = decrypt(encrypted_payment_data, device_key)\n  4. Re-encrypts with service key: service_encrypted = AES-GCM(payment_data, rotating_key)\n  5. Stores: payment_token → service_encrypted\n  6. Returns: payment_token\n```\n\n**Token Decryption (Internal):**\n```\nAuth Worker Request:\n  1. Calls POST /internal/decrypt with payment_token\n  \nPayment Token Service:\n  1. Retrieves service_encrypted from database\n  2. Decrypts with current rotating_key: payment_data = decrypt(service_encrypted, rotating_key)\n  3. Returns: payment_data (in memory only, never persisted)\n```\n\n## API Specification\n\n### POST /payment-tokens\nCreates a payment token from device-encrypted payment data.\n\n**Request:**\n```protobuf\nmessage CreatePaymentTokenRequest {\n  string restaurant_id = 1;  // UUID of the restaurant/merchant\n  bytes encrypted_payment_data = 2;  // Encrypted by POS device with derived key\n  string device_token = 3;  // Device identifier for key derivation\n  string idempotency_key = 4;  // Client-provided idempotency key\n  map<string, string> metadata = 5;  // Optional metadata (card brand, last4, etc.)\n}\n\nmessage CreatePaymentTokenResponse {\n  string payment_token = 1;  // Generated token (UUID format: pt_...)\n  string restaurant_id = 2;\n  int64 expires_at = 3;  // Unix timestamp\n  map<string, string> metadata = 4;  // Echoed metadata\n}\n```\n\n**HTTP:**\n```\nPOST /v1/payment-tokens\nContent-Type: application/x-protobuf\nX-Idempotency-Key: <uuid>\nAuthorization: Bearer <api_key>\n\nResponse:\n201 Created - Token created successfully\n200 OK - Idempotent request, returning existing token\n400 Bad Request - Invalid request or decryption failed\n401 Unauthorized - Invalid API key\n500 Internal Server Error\n```\n\n### GET /payment-tokens/{token_id}\nRetrieves payment token metadata (NOT the actual payment data).\n\n**Request:**\n```protobuf\nmessage GetPaymentTokenRequest {\n  string payment_token = 1;\n  string restaurant_id = 2;  // Must match token owner\n}\n\nmessage GetPaymentTokenResponse {\n  string payment_token = 1;\n  string restaurant_id = 2;\n  int64 created_at = 3;\n  int64 expires_at = 4;\n  bool is_expired = 5;\n  map<string, string> metadata = 6;\n}\n```\n\n**HTTP:**\n```\nGET /v1/payment-tokens/{token_id}?restaurant_id={restaurant_id}\nAuthorization: Bearer <api_key>\n\nResponse:\n200 OK - Token found\n404 Not Found - Token doesn't exist or doesn't belong to restaurant\n410 Gone - Token expired\n```\n\n### POST /internal/decrypt (Internal Only)\nDecrypts a payment token and returns raw payment data. **Only accessible by Auth Processor Workers and Void Workers** within VPC.\n\n**Request:**\n```protobuf\nmessage DecryptPaymentTokenRequest {\n  string payment_token = 1;\n  string restaurant_id = 2;  // For authorization check\n  string requesting_service = 3;  // \"auth-processor-worker\", \"void-processor-worker\"\n}\n\nmessage DecryptPaymentTokenResponse {\n  PaymentData payment_data = 1;\n  map<string, string> metadata = 2;  // card_brand, last4, etc.\n}\n\nmessage PaymentData {\n  string card_number = 1;  // Full PAN\n  string exp_month = 2;  // MM\n  string exp_year = 3;  // YYYY\n  string cvv = 4;\n  string cardholder_name = 5;\n  \n  // Billing address (if available)\n  Address billing_address = 6;\n}\n\nmessage Address {\n  string line1 = 1;\n  string line2 = 2;\n  string city = 3;\n  string state = 4;\n  string postal_code = 5;\n  string country = 6;  // ISO 3166-1 alpha-2\n}\n```\n\n**HTTP:**\n```\nPOST /internal/v1/decrypt\nContent-Type: application/x-protobuf\nX-Service-Auth: <internal-service-token>\nX-Request-ID: <correlation-id>\n\nResponse:\n200 OK - Token decrypted successfully\n400 Bad Request - Invalid token format\n404 Not Found - Token not found\n410 Gone - Token expired\n403 Forbidden - Restaurant ID mismatch or unauthorized service\n500 Internal Server Error\n```\n\n## Behaviors\n\n### B1: Token Creation with Idempotency\n**Given** a client calls POST /payment-tokens with an idempotency key\n**When** the same idempotency key is used within 24 hours\n**Then** the same token is returned (idempotent behavior)\n**And** no duplicate database entries are created\n\n### B2: Device-Based Decryption\n**Given** POS device encrypts payment data with derived key\n**When** Payment Token Service receives encrypted data + device_token\n**Then** derive decryption key using BDK and device_token\n**And** decrypt payment data\n**And** fail with 400 if decryption fails (invalid device_token or corrupted data)\n\n### B3: Re-encryption with Rotating Keys\n**Given** decrypted payment data\n**When** storing in database\n**Then** re-encrypt using current service rotating key\n**And** store key version for future decryption\n**And** support multiple key versions during rotation period\n\n### B4: Token Expiration\n**Given** a payment token is created\n**When** 24 hours have passed (configurable per restaurant)\n**Then** the token is marked as expired and cannot be used for new authorizations\n**And** GET requests return 410 Gone\n**And** Decrypt requests return 410 Gone\n\n### B5: Restaurant Scoping\n**Given** a payment token\n**When** any operation is performed\n**Then** the token can only be accessed by the restaurant that created it\n**And** attempts to access with wrong restaurant_id return 403 Forbidden\n\n### B6: Internal Decryption Authorization\n**Given** a decrypt request to /internal/decrypt\n**When** requesting_service is not in allowlist [\"auth-processor-worker\", \"void-processor-worker\"]\n**Then** return 403 Forbidden\n**And** log security violation\n\n### B7: Audit Logging for Decryption\n**Given** any decrypt request (internal API)\n**When** processed (success or failure)\n**Then** log: payment_token, restaurant_id, requesting_service, timestamp, result\n**And** logs are immutable and retained for 7 years (PCI compliance)\n\n### B8: Key Rotation Support\n**Given** service rotating keys are rotated (e.g., every 90 days)\n**When** old tokens are decrypted\n**Then** use key_version stored with token to select correct decryption key\n**And** support N previous key versions (e.g., 4 versions = 1 year retention)\n**And** background job re-encrypts old tokens with new key\n\n### B9: BDK Security\n**Given** BDK must never leave AWS KMS\n**When** deriving device keys\n**Then** use KMS Decrypt API with encryption context\n**And** derived keys exist only in memory, never persisted\n**And** rotate BDK annually with migration process\n\n## Database Schema\n\n```sql\nCREATE TABLE payment_tokens (\n    payment_token VARCHAR(64) PRIMARY KEY,  -- pt_<uuid>\n    restaurant_id UUID NOT NULL,\n    \n    -- Re-encrypted payment data (service key)\n    encrypted_payment_data BYTEA NOT NULL,\n    encryption_key_version VARCHAR(50) NOT NULL,  -- For key rotation\n    \n    -- Original device info (for audit)\n    device_token VARCHAR(255) NOT NULL,\n    \n    -- Lifecycle\n    created_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    expires_at TIMESTAMP NOT NULL,\n    \n    -- Metadata (non-sensitive)\n    metadata JSONB,  -- card_brand, last4, exp_month (for display)\n    \n    INDEX idx_restaurant_created (restaurant_id, created_at),\n    INDEX idx_expires_at (expires_at) WHERE expires_at > NOW()\n);\n\nCREATE TABLE token_idempotency_keys (\n    idempotency_key VARCHAR(255) NOT NULL,\n    restaurant_id UUID NOT NULL,\n    payment_token VARCHAR(64) NOT NULL REFERENCES payment_tokens(payment_token),\n    created_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    expires_at TIMESTAMP NOT NULL DEFAULT NOW() + INTERVAL '24 hours',\n    \n    PRIMARY KEY (idempotency_key, restaurant_id),\n    INDEX idx_expires_at (expires_at)\n);\n\n-- Encryption key versions (for rotation)\nCREATE TABLE encryption_keys (\n    key_version VARCHAR(50) PRIMARY KEY,\n    kms_key_id VARCHAR(255) NOT NULL,  -- AWS KMS key ARN\n    created_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    is_active BOOLEAN NOT NULL DEFAULT true,\n    retired_at TIMESTAMP,\n    \n    CONSTRAINT only_one_active CHECK (\n        NOT is_active OR \n        (SELECT COUNT(*) FROM encryption_keys WHERE is_active = true) = 1\n    )\n);\n\n-- Audit log for decryption requests (PCI compliance)\nCREATE TABLE decrypt_audit_log (\n    id BIGSERIAL PRIMARY KEY,\n    payment_token VARCHAR(64) NOT NULL,\n    restaurant_id UUID NOT NULL,\n    requesting_service VARCHAR(100) NOT NULL,\n    request_id VARCHAR(255) NOT NULL,  -- Correlation ID\n    success BOOLEAN NOT NULL,\n    error_code VARCHAR(50),\n    created_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    \n    INDEX idx_token_created (payment_token, created_at),\n    INDEX idx_created_at (created_at)\n);\n\n-- Partition audit log by month for archival\nCREATE TABLE decrypt_audit_log_2024_01 PARTITION OF decrypt_audit_log\n    FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');\n```\n\n## Key Derivation Function (KDF)\n\nUse **HKDF (HMAC-based Key Derivation Function)** per RFC 5869:\n\n```python\ndef derive_device_key(bdk: bytes, device_token: str) -> bytes:\n    \"\"\"\n    Derives device-specific encryption key from BDK.\n    \n    Args:\n        bdk: Base Derivation Key (from AWS KMS)\n        device_token: Device identifier\n    \n    Returns:\n        32-byte AES-256 key\n    \"\"\"\n    return HKDF(\n        algorithm=hashes.SHA256(),\n        length=32,\n        salt=None,\n        info=b\"payment-token-v1:\" + device_token.encode('utf-8')\n    ).derive(bdk)\n```\n\n## Dependencies\n- **AWS KMS**: For BDK storage and key management\n- **PostgreSQL**: For token storage (isolated instance)\n- None on other microservices (fully isolated)\n\n## Security Requirements\n- **PCI DSS Level 1** compliance\n- **Separate VPC**/network isolation from other services\n- **Separate database instance** (not shared)\n- **Mutual TLS** for internal /decrypt endpoint\n- **API keys** for public endpoints\n- **BDK** never leaves AWS KMS (use KMS Decrypt API)\n- **Derived keys** exist only in memory during request\n- **Audit logging** for all decrypt operations (immutable, 7-year retention)\n- **Encrypted connections only** (TLS 1.3)\n- **Rate limiting** per restaurant and per service\n\n## Configuration\n\n```yaml\nencryption:\n  bdk_kms_key_id: \"arn:aws:kms:us-east-1:...:key/...\"\n  current_key_version: \"v3\"\n  supported_key_versions: [\"v1\", \"v2\", \"v3\"]\n  key_rotation_days: 90\n\ntokens:\n  default_ttl_hours: 24\n  format: \"pt_{uuid}\"\n\ninternal_api:\n  allowed_services:\n    - \"auth-processor-worker\"\n    - \"void-processor-worker\"\n  require_mtls: true\n\nrate_limiting:\n  per_restaurant_rpm: 1000\n  per_service_rpm: 10000\n```\n\n## Deployment\n- **ECS Service** (long-running, not Lambda due to KMS cold start latency)\n- **Auto-scaling** based on CPU and request rate\n- **Health check**: GET /health\n- **Separate VPC subnet** (PCI zone)\n- **Security groups**: Only allow ingress from API Gateway (public) and Auth Workers (internal)\n\n## Key Rotation Process\n\n### Rotating Service Keys (Every 90 Days)\n1. Generate new key version in AWS KMS\n2. Insert new row in `encryption_keys` table with `is_active = true`\n3. Update old key: `is_active = false, retired_at = NOW()`\n4. New tokens use new key immediately\n5. Background job re-encrypts old tokens over 30 days:\n   ```sql\n   UPDATE payment_tokens\n   SET encrypted_payment_data = re_encrypt(encrypted_payment_data, old_key, new_key),\n       encryption_key_version = 'v4'\n   WHERE encryption_key_version = 'v3'\n   LIMIT 1000;\n   ```\n6. After all tokens re-encrypted, delete old KMS key (90 days later)\n\n### Rotating BDK (Annually)\n1. Generate new BDK in AWS KMS\n2. Update configuration to support both old and new BDK\n3. New tokens use new BDK for device key derivation\n4. Keep old BDK for 24 hours (token TTL)\n5. After 24 hours, retire old BDK\n\n## Testing Strategy\n- **Unit tests**: Key derivation, encryption/decryption, idempotency, expiration\n- **Integration tests**: Full API flow with test database + LocalStack KMS\n- **Security tests**: \n  - Cross-restaurant access attempts\n  - Expired token handling\n  - Invalid device_token (decryption failures)\n  - Unauthorized service access to /internal/decrypt\n- **Key rotation tests**: Decrypt tokens encrypted with old key versions\n- **Load tests**: Token creation throughput (target: 500 RPS)\n- **PCI compliance audit**: Annual third-party assessment\n\n## Monitoring & Alerts\n- **Metrics**: Token creation rate, decrypt request rate, KMS API latency, error rates\n- **Alarms**:\n  - Decryption failure rate > 1%\n  - KMS throttling errors\n  - Unauthorized decrypt attempts > 10/min\n- **Audit**: Daily review of decrypt_audit_log for anomalies\n","priority":0,"archived":0,"archived_at":null,"created_at":"2025-11-10 04:35:29","updated_at":"2025-11-10 06:06:22","parent_id":null,"parent_uuid":null,"relationships":[{"from":"s-7ujm","from_type":"spec","to":"s-8c0t","to_type":"spec","type":"references"}],"tags":["pci-compliant","service-spec","tokenization"]}
{"id":"s-9jeq","uuid":"152dae2a-283e-476c-926b-785cc96de8a4","title":"Authorization API Service","file_path":"specs/authorization_api_service.md","content":"## Overview\nPrimary API service for handling payment authorization requests. Implements event sourcing with **Transactional Outbox Pattern**, idempotency, and request/response polling. This is the main entry point for restaurant POS systems.\n\n## Service Boundaries\n- **Owns**: Auth request lifecycle, event storage, read model updates, idempotency, status queries, void requests, outbox processing\n- **Does NOT own**: Actual payment processing (delegated to workers), token decryption, processor integration\n- **Responsibilities**: API layer, event sourcing, read model maintenance (synchronous), queue management via outbox\n\n## Transaction Boundaries & Outbox Pattern\n\n### Critical Design Decision: At-Least-Once Delivery via Outbox\n\nTo ensure **atomic** event writes + queue messages, we use the **Transactional Outbox Pattern**:\n\n```\n┌─────────────────────────────────────────────────────────┐\n│                  POST /authorize                         │\n│                                                          │\n│  BEGIN TRANSACTION                                       │\n│    1. Check idempotency                                  │\n│    2. Write event: AuthRequestCreated                    │\n│    3. Update read model: auth_request_state (INSERT)     │\n│    4. Write to outbox: auth_request_queued               │\n│  COMMIT (all or nothing!)                                │\n│                                                          │\n│  Return auth_request_id to client                        │\n└─────────────────────────────────────────────────────────┘\n                          │\n                          │ (asynchronously)\n                          ▼\n┌─────────────────────────────────────────────────────────┐\n│              Outbox Processor (background)               │\n│                                                          │\n│  Every 100ms:                                            │\n│    1. SELECT * FROM outbox WHERE processed_at IS NULL    │\n│       FOR UPDATE SKIP LOCKED LIMIT 100                   │\n│    2. Send each message to SQS                           │\n│    3. UPDATE outbox SET processed_at = NOW()             │\n└─────────────────────────────────────────────────────────┘\n```\n\n**Guarantees:**\n- ✅ Event + Read Model + Outbox written atomically\n- ✅ At-least-once delivery to SQS (outbox processor retries until success)\n- ✅ No message loss (even if outbox processor crashes, messages remain in DB)\n- ✅ SQS FIFO deduplication handles any duplicate sends\n\n## API Specification\n\n### POST /authorize\nCreates an authorization request and waits up to 5 seconds for completion.\n\n**Request:**\n```protobuf\nmessage AuthorizeRequest {\n  string payment_token = 1;  // From Payment Token Service\n  string restaurant_id = 2;  // UUID of restaurant\n  int64 amount_cents = 3;  // Amount in cents (e.g., 1050 = $10.50)\n  string currency = 4;  // ISO 4217 currency code (e.g., \"USD\")\n  string idempotency_key = 5;  // Client-provided\n  map<string, string> metadata = 6;  // Optional: order_id, table_number, etc.\n}\n\nmessage AuthorizeResponse {\n  string auth_request_id = 1;  // UUID\n  AuthStatus status = 2;\n  \n  // Populated if status = AUTHORIZED or DENIED\n  AuthorizationResult result = 3;\n  \n  // Populated if status = PROCESSING\n  string status_url = 4;  // URL to poll for status\n}\n\nenum AuthStatus {\n  PROCESSING = 0;  // Still in progress\n  AUTHORIZED = 1;  // Successfully authorized\n  DENIED = 2;  // Declined by processor\n  FAILED = 3;  // System error\n  VOIDED = 4;  // Voided by user\n  EXPIRED = 5;  // Request expired before processing\n}\n\nmessage AuthorizationResult {\n  string processor_auth_id = 1;  // e.g., Stripe charge ID\n  string processor_name = 2;  // \"stripe\", \"chase\", etc.\n  int64 authorized_amount_cents = 3;\n  string currency = 4;\n  string authorization_code = 5;  // Processor-provided auth code\n  int64 authorized_at = 6;  // Unix timestamp\n  \n  // For DENIED status\n  string denial_code = 7;  // Processor decline code\n  string denial_reason = 8;  // Human-readable reason\n  \n  map<string, string> processor_metadata = 9;  // Raw processor response\n}\n```\n\n**HTTP:**\n```\nPOST /v1/authorize\nContent-Type: application/x-protobuf\nX-Idempotency-Key: <uuid>\n\nResponse:\n200 OK - Authorization completed within 5 seconds (AUTHORIZED/DENIED)\n202 Accepted - Authorization still processing (client should poll)\n400 Bad Request - Invalid request\n404 Not Found - Payment token not found/expired\n409 Conflict - Auth request already voided\n500 Internal Server Error\n```\n\n**Implementation (Pseudocode):**\n```python\nasync def post_authorize(request: AuthorizeRequest) -> AuthorizeResponse:\n    # 1. Check idempotency\n    existing = await db.fetch_one(\n        \"SELECT auth_request_id FROM auth_idempotency_keys WHERE idempotency_key = $1\",\n        request.idempotency_key\n    )\n    if existing:\n        return await get_status(existing.auth_request_id)\n    \n    auth_request_id = generate_uuid()\n    \n    # 2. ATOMIC TRANSACTION\n    async with db.transaction():\n        # 2a. Write event\n        await db.execute(\"\"\"\n            INSERT INTO payment_events (event_id, aggregate_id, aggregate_type, event_type, event_data, sequence_number)\n            VALUES ($1, $2, 'auth_request', 'AuthRequestCreated', $3, 1)\n        \"\"\", generate_uuid(), auth_request_id, serialize_protobuf(AuthRequestCreated(...)))\n        \n        # 2b. Write read model (strong consistency!)\n        await db.execute(\"\"\"\n            INSERT INTO auth_request_state (auth_request_id, restaurant_id, payment_token, status, amount_cents, currency, created_at, updated_at, last_event_sequence)\n            VALUES ($1, $2, $3, 'PENDING', $4, $5, NOW(), NOW(), 1)\n        \"\"\", auth_request_id, request.restaurant_id, request.payment_token, request.amount_cents, request.currency)\n        \n        # 2c. Write to outbox (reliable queue delivery!)\n        await db.execute(\"\"\"\n            INSERT INTO outbox (aggregate_id, message_type, payload, created_at)\n            VALUES ($1, 'auth_request_queued', $2, NOW())\n        \"\"\", auth_request_id, json.dumps({\n            \"auth_request_id\": auth_request_id,\n            \"restaurant_id\": request.restaurant_id\n        }))\n        \n        # 2d. Write idempotency key\n        await db.execute(\"\"\"\n            INSERT INTO auth_idempotency_keys (idempotency_key, auth_request_id, restaurant_id, created_at, expires_at)\n            VALUES ($1, $2, $3, NOW(), NOW() + INTERVAL '24 hours')\n        \"\"\", request.idempotency_key, auth_request_id, request.restaurant_id)\n        \n        # COMMIT - all or nothing!\n    \n    # 3. Wait up to 5 seconds for response (poll read model)\n    for _ in range(50):  # 50 * 100ms = 5 seconds\n        await asyncio.sleep(0.1)\n        \n        state = await db.fetch_one(\n            \"SELECT status, processor_auth_id, authorization_code, denial_code, denial_reason FROM auth_request_state WHERE auth_request_id = $1\",\n            auth_request_id\n        )\n        \n        if state.status in ('AUTHORIZED', 'DENIED', 'FAILED'):\n            return AuthorizeResponse(\n                auth_request_id=auth_request_id,\n                status=state.status,\n                result=AuthorizationResult(...) if state.status != 'FAILED' else None\n            )\n    \n    # 4. Timeout - return 202 Accepted\n    return AuthorizeResponse(\n        auth_request_id=auth_request_id,\n        status=AuthStatus.PROCESSING,\n        status_url=f\"/v1/authorize/{auth_request_id}/status\"\n    )\n```\n\n### GET /authorize/{auth_request_id}/status\nPolls for authorization status (reads from read model).\n\n**Request:**\n```protobuf\nmessage GetAuthStatusRequest {\n  string auth_request_id = 1;\n  string restaurant_id = 2;  // Must match\n}\n\nmessage GetAuthStatusResponse {\n  string auth_request_id = 1;\n  AuthStatus status = 2;\n  AuthorizationResult result = 3;  // If completed\n  int64 created_at = 4;\n  int64 updated_at = 5;\n}\n```\n\n**HTTP:**\n```\nGET /v1/authorize/{auth_request_id}/status?restaurant_id={restaurant_id}\n\nResponse:\n200 OK - Status retrieved\n404 Not Found - Auth request not found or wrong restaurant\n```\n\n**Implementation:**\n```python\nasync def get_status(auth_request_id: str, restaurant_id: str) -> GetAuthStatusResponse:\n    # Simple read from read model (no transaction needed)\n    state = await db.fetch_one(\"\"\"\n        SELECT auth_request_id, status, processor_auth_id, processor_name, \n               authorized_amount_cents, authorization_code, denial_code, denial_reason,\n               created_at, updated_at\n        FROM auth_request_state\n        WHERE auth_request_id = $1 AND restaurant_id = $2\n    \"\"\", auth_request_id, restaurant_id)\n    \n    if not state:\n        raise NotFound(\"Auth request not found\")\n    \n    return GetAuthStatusResponse(\n        auth_request_id=state.auth_request_id,\n        status=state.status,\n        result=build_result(state) if state.status in ('AUTHORIZED', 'DENIED') else None,\n        created_at=state.created_at,\n        updated_at=state.updated_at\n    )\n```\n\n### POST /authorize/{auth_request_id}/void\nVoids an authorization request (similar transaction pattern).\n\n**Request:**\n```protobuf\nmessage VoidAuthRequest {\n  string auth_request_id = 1;\n  string restaurant_id = 2;\n  string reason = 3;  // Optional: \"customer_cancelled\", etc.\n  string idempotency_key = 4;\n}\n\nmessage VoidAuthResponse {\n  string auth_request_id = 1;\n  VoidStatus status = 2;\n  int64 voided_at = 3;\n}\n\nenum VoidStatus {\n  VOID_PENDING = 0;  // Void request queued\n  VOID_COMPLETED = 1;  // Void successful\n  VOID_FAILED = 2;  // Could not void (already captured, etc.)\n  VOID_NOT_REQUIRED = 3;  // Never authorized, just cancelled queue\n}\n```\n\n**HTTP:**\n```\nPOST /v1/authorize/{auth_request_id}/void\nContent-Type: application/x-protobuf\n\nResponse:\n200 OK - Void processed\n404 Not Found - Auth request not found\n409 Conflict - Already voided or captured\n```\n\n**Implementation:**\n```python\nasync def post_void(auth_request_id: str, request: VoidAuthRequest) -> VoidAuthResponse:\n    async with db.transaction():\n        # 1. Get current state\n        state = await db.fetch_one(\n            \"SELECT status FROM auth_request_state WHERE auth_request_id = $1 AND restaurant_id = $2 FOR UPDATE\",\n            auth_request_id, request.restaurant_id\n        )\n        \n        if not state:\n            raise NotFound(\"Auth request not found\")\n        \n        if state.status == 'VOIDED':\n            raise Conflict(\"Already voided\")\n        \n        # 2. Write void event\n        next_seq = await db.fetch_val(\n            \"SELECT MAX(sequence_number) + 1 FROM payment_events WHERE aggregate_id = $1\",\n            auth_request_id\n        )\n        \n        await db.execute(\"\"\"\n            INSERT INTO payment_events (event_id, aggregate_id, aggregate_type, event_type, event_data, sequence_number)\n            VALUES ($1, $2, 'auth_request', 'AuthVoidRequested', $3, $4)\n        \"\"\", generate_uuid(), auth_request_id, serialize_protobuf(AuthVoidRequested(...)), next_seq)\n        \n        # 3. Update read model\n        await db.execute(\"\"\"\n            UPDATE auth_request_state\n            SET status = 'VOIDED', updated_at = NOW(), last_event_sequence = $2\n            WHERE auth_request_id = $1\n        \"\"\", auth_request_id, next_seq)\n        \n        # 4. If AUTHORIZED, write to outbox for void worker\n        if state.status == 'AUTHORIZED':\n            await db.execute(\"\"\"\n                INSERT INTO outbox (aggregate_id, message_type, payload, created_at)\n                VALUES ($1, 'void_request_queued', $2, NOW())\n            \"\"\", auth_request_id, json.dumps({\n                \"auth_request_id\": auth_request_id,\n                \"restaurant_id\": request.restaurant_id,\n                \"reason\": request.reason\n            }))\n        \n        # COMMIT\n    \n    return VoidAuthResponse(\n        auth_request_id=auth_request_id,\n        status=VoidStatus.VOID_PENDING if state.status == 'AUTHORIZED' else VoidStatus.VOID_NOT_REQUIRED,\n        voided_at=int(time.time())\n    )\n```\n\n## Outbox Processor (Background Service)\n\n**Runs as separate process/thread within Authorization API deployment.**\n\n```python\nasync def outbox_processor():\n    \"\"\"\n    Background process that polls outbox and sends messages to SQS.\n    Runs continuously every 100ms.\n    \"\"\"\n    while True:\n        try:\n            # 1. Fetch unprocessed messages (with locking to prevent duplicate processing)\n            messages = await db.fetch(\"\"\"\n                SELECT id, aggregate_id, message_type, payload\n                FROM outbox\n                WHERE processed_at IS NULL\n                ORDER BY created_at\n                LIMIT 100\n                FOR UPDATE SKIP LOCKED\n            \"\"\")\n            \n            for msg in messages:\n                try:\n                    # 2. Send to appropriate SQS queue\n                    if msg.message_type == 'auth_request_queued':\n                        await sqs.send_message(\n                            QueueUrl=AUTH_REQUESTS_QUEUE_URL,\n                            MessageBody=msg.payload,\n                            MessageDeduplicationId=msg.aggregate_id,  # SQS FIFO deduplication\n                            MessageGroupId=json.loads(msg.payload)[\"restaurant_id\"]\n                        )\n                    elif msg.message_type == 'void_request_queued':\n                        await sqs.send_message(\n                            QueueUrl=VOID_REQUESTS_QUEUE_URL,\n                            MessageBody=msg.payload\n                        )\n                    \n                    # 3. Mark as processed\n                    await db.execute(\n                        \"UPDATE outbox SET processed_at = NOW() WHERE id = $1\",\n                        msg.id\n                    )\n                    \n                except Exception as e:\n                    # Log error but continue (will retry on next poll)\n                    logger.error(f\"Failed to process outbox message {msg.id}: {e}\")\n            \n            # 4. Sleep 100ms\n            await asyncio.sleep(0.1)\n            \n        except Exception as e:\n            logger.error(f\"Outbox processor error: {e}\")\n            await asyncio.sleep(1)  # Back off on error\n```\n\n## Event Definitions\n\nAll events stored in `payment_events` table.\n\n```protobuf\n// Event: AuthRequestCreated\nmessage AuthRequestCreated {\n  string auth_request_id = 1;\n  string payment_token = 2;\n  string restaurant_id = 3;\n  int64 amount_cents = 4;\n  string currency = 5;\n  map<string, string> metadata = 6;\n  int64 created_at = 7;\n}\n\n// Event: AuthRequestQueued (outbox message type, not an event)\n// Stored as JSON in outbox.payload\n\n// Event: AuthAttemptStarted (written by worker)\nmessage AuthAttemptStarted {\n  string auth_request_id = 1;\n  string worker_id = 2;  // Which worker is processing\n  string restaurant_payment_config_version = 3;\n  int64 started_at = 4;\n}\n\n// Event: AuthResponseReceived (written by worker)\nmessage AuthResponseReceived {\n  string auth_request_id = 1;\n  AuthStatus status = 2;  // AUTHORIZED or DENIED\n  AuthorizationResult result = 3;\n  int64 received_at = 4;\n}\n\n// Event: AuthAttemptFailed (written by worker)\nmessage AuthAttemptFailed {\n  string auth_request_id = 1;\n  string error_code = 2;  // \"token_invalid\", \"processor_timeout\", etc.\n  string error_message = 3;\n  bool is_retryable = 4;\n  int32 retry_count = 5;\n  int64 next_retry_at = 6;  // If retryable\n  int64 failed_at = 7;\n}\n\n// Event: AuthVoidRequested\nmessage AuthVoidRequested {\n  string auth_request_id = 1;\n  string reason = 2;\n  int64 requested_at = 3;\n}\n\n// Event: AuthRequestExpired (written by worker or timeout process)\nmessage AuthRequestExpired {\n  string auth_request_id = 1;\n  int64 expired_at = 2;\n  string reason = 3;  // \"timeout\", \"max_retries_exceeded\", \"voided_before_processing\"\n}\n```\n\n## Behaviors\n\n### B1: Idempotency\n**Given** POST /authorize with idempotency key \"xyz\"\n**When** called multiple times\n**Then** same auth_request_id is returned\n**And** only ONE event is created\n**And** only ONE outbox entry is created\n**And** only ONE SQS message is sent\n\n### B2: Synchronous Response (Fast Path)\n**Given** POST /authorize is called\n**When** processor responds within 5 seconds\n**Then** return 200 with AUTHORIZED or DENIED status\n**And** do NOT return status_url\n\n### B3: Asynchronous Response (Slow Path)\n**Given** POST /authorize is called\n**When** 5 seconds pass without response\n**Then** return 202 Accepted with status_url\n**And** client should poll GET /authorize/{id}/status\n\n### B4: Atomic Event + Read Model + Outbox\n**Given** POST /authorize is called\n**When** transaction commits\n**Then** event, read model, and outbox entry are ALL written atomically\n**When** transaction fails/rolls back\n**Then** NONE are written (no partial state)\n\n### B5: Outbox Guarantees At-Least-Once Delivery\n**Given** outbox entry is written\n**When** outbox processor runs\n**Then** message is sent to SQS at least once\n**And** SQS FIFO deduplication prevents duplicate processing\n**And** even if processor crashes, message remains in outbox and is retried\n\n### B6: Read Model is Strongly Consistent\n**Given** transaction commits with read model update\n**When** GET /authorize/{id}/status is called immediately\n**Then** latest status is returned (no eventual consistency delay)\n\n### B7: Void Before Processing\n**Given** auth request is PENDING or PROCESSING\n**When** void is requested\n**Then** emit AuthVoidRequested event\n**And** update read model status to VOIDED\n**And** worker checks for void event before calling processor\n**And** return VOID_NOT_REQUIRED (no processor call needed)\n\n### B8: Void After Authorization\n**Given** auth request is AUTHORIZED\n**When** void is requested\n**Then** emit AuthVoidRequested event\n**And** update read model status to VOIDED\n**And** enqueue void request to void queue via outbox\n**And** void worker processes void with retries\n\n### B9: Dead Letter - Invalid Token (handled by worker)\n**Given** auth request with invalid/expired token\n**When** worker attempts to process\n**Then** worker writes AuthAttemptFailed event\n**And** worker updates read model status to FAILED\n**And** move to dead letter queue\n\n### B10: Restaurant Payment Config Changes\n**Given** restaurant config changes (e.g., switch from Stripe to Chase)\n**When** queued auth request is processed\n**Then** worker uses CURRENT config at processing time (optimistic read)\n**And** allows replaying with new config\n\n## Database Schema\n\n```sql\n-- Event store (append-only)\nCREATE TABLE payment_events (\n    id BIGSERIAL PRIMARY KEY,\n    event_id UUID UNIQUE NOT NULL,\n    aggregate_id UUID NOT NULL,  -- auth_request_id\n    aggregate_type VARCHAR(50) NOT NULL DEFAULT 'auth_request',\n    event_type VARCHAR(100) NOT NULL,  -- 'AuthRequestCreated', etc.\n    event_data BYTEA NOT NULL,  -- Serialized protobuf\n    metadata JSONB,  -- correlation_id, causation_id, user_id\n    created_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    sequence_number INTEGER NOT NULL,\n    UNIQUE(aggregate_id, sequence_number)\n);\n\nCREATE INDEX idx_aggregate_events ON payment_events(aggregate_id, sequence_number);\nCREATE INDEX idx_event_type_created ON payment_events(event_type, created_at);\nCREATE INDEX idx_created_at ON payment_events(created_at);\n\n-- Read model for fast queries (updated synchronously in same transaction as event)\nCREATE TABLE auth_request_state (\n    auth_request_id UUID PRIMARY KEY,\n    restaurant_id UUID NOT NULL,\n    payment_token VARCHAR(64) NOT NULL,\n    status VARCHAR(20) NOT NULL,  -- PENDING, PROCESSING, AUTHORIZED, etc.\n    amount_cents BIGINT NOT NULL,\n    currency VARCHAR(3) NOT NULL,\n    \n    -- Result (populated when completed by worker)\n    processor_auth_id VARCHAR(255),\n    processor_name VARCHAR(50),\n    authorized_amount_cents BIGINT,\n    authorization_code VARCHAR(100),\n    denial_code VARCHAR(50),\n    denial_reason TEXT,\n    \n    created_at TIMESTAMP NOT NULL,\n    updated_at TIMESTAMP NOT NULL,\n    completed_at TIMESTAMP,\n    \n    metadata JSONB,\n    last_event_sequence INTEGER NOT NULL,  -- For consistency check\n    \n    INDEX idx_restaurant_created (restaurant_id, created_at),\n    INDEX idx_status (status),\n    INDEX idx_payment_token (payment_token)\n);\n\n-- Outbox for reliable queue delivery (Transactional Outbox Pattern)\nCREATE TABLE outbox (\n    id BIGSERIAL PRIMARY KEY,\n    aggregate_id UUID NOT NULL,  -- auth_request_id or void_request_id\n    message_type VARCHAR(100) NOT NULL,  -- 'auth_request_queued', 'void_request_queued'\n    payload JSONB NOT NULL,  -- Message body for SQS\n    created_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    processed_at TIMESTAMP,  -- NULL = unprocessed\n    \n    INDEX idx_unprocessed (created_at) WHERE processed_at IS NULL  -- Partial index for efficiency!\n);\n\n-- Idempotency keys\nCREATE TABLE auth_idempotency_keys (\n    idempotency_key VARCHAR(255) PRIMARY KEY,\n    auth_request_id UUID NOT NULL REFERENCES auth_request_state(auth_request_id),\n    restaurant_id UUID NOT NULL,\n    created_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    expires_at TIMESTAMP NOT NULL DEFAULT NOW() + INTERVAL '24 hours',\n    INDEX idx_expires_at (expires_at)\n);\n\n-- Restaurant payment configs (mocked for now, single Stripe config)\nCREATE TABLE restaurant_payment_configs (\n    restaurant_id UUID PRIMARY KEY,\n    config_version VARCHAR(50) NOT NULL,\n    processor_name VARCHAR(50) NOT NULL,  -- \"stripe\"\n    processor_config JSONB NOT NULL,  -- {\"stripe_api_key\": \"sk_test_...\"}\n    updated_at TIMESTAMP NOT NULL,\n    is_active BOOLEAN NOT NULL DEFAULT true\n);\n\n-- Lock table for workers (used by Auth Processor Worker)\nCREATE TABLE auth_processing_locks (\n    auth_request_id UUID PRIMARY KEY,\n    worker_id VARCHAR(255) NOT NULL,\n    locked_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    expires_at TIMESTAMP NOT NULL,\n    INDEX idx_expires_at (expires_at)\n);\n```\n\n## Dependencies\n- **Payment Token Service**: Validates token exists (not directly called by this service)\n- **SQS FIFO Queue**: Receives auth requests from outbox processor\n- **PostgreSQL**: Event store, read models, outbox\n- **Auth Processor Workers**: Processes queued requests (separate service)\n\n## Queue Configuration\n\n**Auth Request Queue** (SQS FIFO):\n- Name: `auth-requests.fifo`\n- Deduplication: Message deduplication ID = auth_request_id\n- Message Group ID: restaurant_id (ordering per restaurant)\n- Visibility timeout: 30 seconds\n- Max receive count: 5 → Dead letter queue\n\n**Void Request Queue** (SQS Standard):\n- Name: `void-requests`\n- Retries with exponential backoff\n\n**Dead Letter Queues**:\n- `auth-requests-dlq.fifo`: Terminal auth failures\n- `void-requests-dlq`: Failed voids\n\n## Background Jobs\n\n### Outbox Processor\n- **Frequency**: Every 100ms\n- **Batch size**: 100 messages\n- **Responsibility**: Poll outbox, send to SQS, mark as processed\n\n### Expired Idempotency Keys Cleanup\n- **Frequency**: Hourly\n- **Query**: `DELETE FROM auth_idempotency_keys WHERE expires_at < NOW()`\n\n### Expired Locks Cleanup (redundant with worker cleanup, but good hygiene)\n- **Frequency**: Every minute\n- **Query**: `DELETE FROM auth_processing_locks WHERE expires_at < NOW()`\n\n## Deployment\n- **ECS Service** or **Lambda**: API handlers\n- **Background thread/process**: Outbox processor (within same deployment)\n- **Auto-scaling**: Based on request rate\n- **Health check**: GET /health (includes outbox processor health)\n\n## Testing Strategy\n- **Unit tests**: \n  - Transaction logic (event + read model + outbox atomic writes)\n  - Idempotency\n  - State reconstruction from events\n- **Integration tests**: \n  - Full API flow with test database\n  - Outbox processor sends to LocalStack SQS\n  - Verify at-least-once delivery\n- **Behavior tests**: Each behavior spec (B1-B10) has dedicated test\n- **End-to-end tests**: POST /authorize → outbox → SQS → worker processes → GET status returns result\n- **Chaos tests**: Kill outbox processor mid-processing, verify messages are retried\n- **Load tests**: 300 QPS sustained, measure p50/p95/p99 latency","priority":0,"archived":0,"archived_at":null,"created_at":"2025-11-10 04:35:30","updated_at":"2025-11-10 06:40:29","parent_id":null,"parent_uuid":null,"relationships":[{"from":"s-9jeq","from_type":"spec","to":"s-94si","to_type":"spec","type":"implements"},{"from":"s-9jeq","from_type":"spec","to":"s-8c0t","to_type":"spec","type":"depends-on"}],"tags":["api","event-sourcing","service-spec"]}
{"id":"s-w5sf","uuid":"6ed0e286-c807-4bb7-902f-c6975f4fb75b","title":"Auth Processor Worker Service","file_path":"specs/auth_processor_worker_service.md","content":"## Overview\nBackground worker service that dequeues authorization requests, calls external services (payment token service for decryption, payment processors), and records results. Implements exactly-once processing with distributed locking and **atomic event + read model updates**.\n\n## Service Boundaries\n- **Owns**: Auth request processing, processor integration, retry logic, locking, **read model updates for worker events**\n- **Does NOT own**: API layer, event storage schema, token encryption/decryption logic\n- **Responsibilities**: Dequeue, lock, process, record result (event + read model), unlock\n\n## Transaction Boundaries\n\n### Critical: Worker Updates Read Model Atomically\n\nWorkers write events AND update the read model in the **same transaction**:\n\n```\n┌─────────────────────────────────────────────────────────┐\n│            Worker Processes Auth Request                 │\n│                                                          │\n│  BEGIN TRANSACTION                                       │\n│    1. Write event: AuthResponseReceived                  │\n│    2. Update read model: auth_request_state              │\n│       SET status = 'AUTHORIZED', processor_auth_id = ... │\n│  COMMIT (all or nothing!)                                │\n│                                                          │\n│  DELETE SQS message                                      │\n└─────────────────────────────────────────────────────────┘\n```\n\n**Guarantees:**\n- ✅ Event + Read Model written atomically\n- ✅ If transaction fails, SQS message remains visible (will retry)\n- ✅ Read model always reflects latest event\n- ✅ No eventual consistency delay\n\n## Worker Architecture\n\n```\n┌─────────────────────────────────────────────────────────┐\n│         SQS FIFO Consumer (Auth Request Queue)          │\n│  - Long polling (20 second wait)                        │\n│  - Batch size: 1 (for simplicity, can optimize later)  │\n│  - Visibility timeout: 30 seconds                       │\n└─────────────────────────────────────────────────────────┘\n                        │\n                        ▼\n┌─────────────────────────────────────────────────────────┐\n│              Acquire Distributed Lock                    │\n│  - INSERT INTO auth_processing_locks                    │\n│  - If lock exists and not expired → skip (race)         │\n│  - Lock TTL: 30 seconds                                 │\n└─────────────────────────────────────────────────────────┘\n                        │\n                        ▼\n┌─────────────────────────────────────────────────────────┐\n│           Check for Void Event (Race Check)             │\n│  - Query: SELECT event_type = 'AuthVoidRequested'      │\n│  - If found → skip processing, delete from queue        │\n└─────────────────────────────────────────────────────────┘\n                        │\n                        ▼\n┌─────────────────────────────────────────────────────────┐\n│      Fetch Restaurant Payment Config (Optimistic)       │\n│  - Read from cached table: restaurant_payment_configs   │\n│  - Determines which processor to use                    │\n└─────────────────────────────────────────────────────────┘\n                        │\n                        ▼\n┌─────────────────────────────────────────────────────────┐\n│    Call Payment Token Service /internal/decrypt         │\n│  - POST /internal/decrypt                               │\n│  - Returns decrypted payment data                       │\n└─────────────────────────────────────────────────────────┘\n                        │\n                        ▼\n┌─────────────────────────────────────────────────────────┐\n│        Call Payment Processor (Stripe, Chase, etc)      │\n│  - Stripe: POST /v1/charges                             │\n│  - Handle response: success, decline, error             │\n└─────────────────────────────────────────────────────────┘\n                        │\n                        ▼\n┌─────────────────────────────────────────────────────────┐\n│     ATOMIC: Write Event + Update Read Model             │\n│  BEGIN TRANSACTION                                       │\n│    - Write event: AuthResponseReceived                   │\n│    - Update auth_request_state                          │\n│  COMMIT                                                  │\n└─────────────────────────────────────────────────────────┘\n                        │\n                        ▼\n┌─────────────────────────────────────────────────────────┐\n│              Release Lock & Delete Message              │\n│  - DELETE FROM auth_processing_locks                    │\n│  - SQS DeleteMessage (ack)                              │\n└─────────────────────────────────────────────────────────┘\n```\n\n## Processing Logic (Pseudocode)\n\n```python\nasync def process_auth_request(message: SQSMessage):\n    auth_request_id = message.body['auth_request_id']\n    \n    # 1. Acquire lock\n    lock_acquired = await acquire_lock(auth_request_id)\n    if not lock_acquired:\n        # Another worker is processing, skip\n        return\n    \n    try:\n        # 2. Check for void (race condition check)\n        if await is_voided(auth_request_id):\n            # Write void completion event + update read model atomically\n            async with db.transaction():\n                await record_event(AuthRequestExpired(\n                    auth_request_id=auth_request_id,\n                    reason=\"voided_before_processing\"\n                ))\n                await db.execute(\"\"\"\n                    UPDATE auth_request_state\n                    SET status = 'EXPIRED', updated_at = NOW()\n                    WHERE auth_request_id = $1\n                \"\"\", auth_request_id)\n            \n            await delete_message(message)\n            return\n        \n        # 3. Emit processing event + update read model atomically\n        async with db.transaction():\n            next_seq = await get_next_sequence(auth_request_id)\n            await record_event(AuthAttemptStarted(\n                auth_request_id=auth_request_id,\n                worker_id=WORKER_ID,\n                started_at=now()\n            ), sequence=next_seq)\n            \n            await db.execute(\"\"\"\n                UPDATE auth_request_state\n                SET status = 'PROCESSING', updated_at = NOW(), last_event_sequence = $2\n                WHERE auth_request_id = $1\n            \"\"\", auth_request_id, next_seq)\n        \n        # 4. Fetch auth request details\n        auth_req = await get_auth_request(auth_request_id)\n        \n        # 5. Fetch restaurant config (optimistic)\n        config = await get_restaurant_config(auth_req.restaurant_id)\n        \n        # 6. Decrypt payment token via Payment Token Service\n        try:\n            payment_data = await payment_token_service.decrypt(\n                payment_token=auth_req.payment_token,\n                restaurant_id=auth_req.restaurant_id,\n                requesting_service=\"auth-processor-worker\"\n            )\n        except TokenNotFound:\n            # Terminal error - write event + update read model + dead letter\n            async with db.transaction():\n                next_seq = await get_next_sequence(auth_request_id)\n                await record_event(AuthAttemptFailed(\n                    auth_request_id=auth_request_id,\n                    error_code=\"token_invalid\",\n                    is_retryable=False\n                ), sequence=next_seq)\n                \n                await db.execute(\"\"\"\n                    UPDATE auth_request_state\n                    SET status = 'FAILED', updated_at = NOW(), last_event_sequence = $2\n                    WHERE auth_request_id = $1\n                \"\"\", auth_request_id, next_seq)\n            \n            await send_to_dlq(message)\n            await delete_message(message)\n            return\n        \n        except TokenExpired:\n            # Terminal error - write event + update read model + dead letter\n            async with db.transaction():\n                next_seq = await get_next_sequence(auth_request_id)\n                await record_event(AuthAttemptFailed(\n                    auth_request_id=auth_request_id,\n                    error_code=\"token_expired\",\n                    is_retryable=False\n                ), sequence=next_seq)\n                \n                await db.execute(\"\"\"\n                    UPDATE auth_request_state\n                    SET status = 'FAILED', updated_at = NOW(), last_event_sequence = $2\n                    WHERE auth_request_id = $1\n                \"\"\", auth_request_id, next_seq)\n            \n            await send_to_dlq(message)\n            await delete_message(message)\n            return\n        \n        # 7. Call processor\n        processor = get_processor_client(config.processor_name)\n        try:\n            result = await processor.authorize(\n                payment_data=payment_data,\n                amount_cents=auth_req.amount_cents,\n                currency=auth_req.currency,\n                config=config.processor_config\n            )\n            \n            # 8. ATOMIC: Record event + update read model\n            async with db.transaction():\n                next_seq = await get_next_sequence(auth_request_id)\n                \n                # Write event\n                await record_event(AuthResponseReceived(\n                    auth_request_id=auth_request_id,\n                    status=result.status,  # AUTHORIZED or DENIED\n                    result=result\n                ), sequence=next_seq)\n                \n                # Update read model\n                if result.status == AuthStatus.AUTHORIZED:\n                    await db.execute(\"\"\"\n                        UPDATE auth_request_state\n                        SET status = 'AUTHORIZED',\n                            processor_auth_id = $2,\n                            processor_name = $3,\n                            authorized_amount_cents = $4,\n                            authorization_code = $5,\n                            completed_at = NOW(),\n                            updated_at = NOW(),\n                            last_event_sequence = $6\n                        WHERE auth_request_id = $1\n                    \"\"\", auth_request_id, result.processor_auth_id, result.processor_name,\n                         result.authorized_amount_cents, result.authorization_code, next_seq)\n                \n                elif result.status == AuthStatus.DENIED:\n                    await db.execute(\"\"\"\n                        UPDATE auth_request_state\n                        SET status = 'DENIED',\n                            processor_name = $2,\n                            denial_code = $3,\n                            denial_reason = $4,\n                            completed_at = NOW(),\n                            updated_at = NOW(),\n                            last_event_sequence = $5\n                        WHERE auth_request_id = $1\n                    \"\"\", auth_request_id, result.processor_name, result.denial_code,\n                         result.denial_reason, next_seq)\n                \n                # COMMIT - event + read model together!\n            \n            # 9. Delete from queue (after successful commit)\n            await delete_message(message)\n            \n        except ProcessorTimeout:\n            # Retryable error - record attempt but don't update status to FAILED yet\n            retry_count = message.attributes['ApproximateReceiveCount']\n            \n            if retry_count >= MAX_RETRIES:\n                # Max retries exceeded - terminal failure\n                async with db.transaction():\n                    next_seq = await get_next_sequence(auth_request_id)\n                    await record_event(AuthAttemptFailed(\n                        auth_request_id=auth_request_id,\n                        error_code=\"processor_timeout\",\n                        is_retryable=False,\n                        retry_count=retry_count\n                    ), sequence=next_seq)\n                    \n                    await db.execute(\"\"\"\n                        UPDATE auth_request_state\n                        SET status = 'FAILED', updated_at = NOW(), last_event_sequence = $2\n                        WHERE auth_request_id = $1\n                    \"\"\", auth_request_id, next_seq)\n                \n                await send_to_dlq(message)\n                await delete_message(message)\n            else:\n                # Let it retry - just record the attempt\n                async with db.transaction():\n                    next_seq = await get_next_sequence(auth_request_id)\n                    await record_event(AuthAttemptFailed(\n                        auth_request_id=auth_request_id,\n                        error_code=\"processor_timeout\",\n                        is_retryable=True,\n                        retry_count=retry_count,\n                        next_retry_at=now() + exponential_backoff(retry_count)\n                    ), sequence=next_seq)\n                    \n                    # Don't update status - keep as PROCESSING\n                    await db.execute(\"\"\"\n                        UPDATE auth_request_state\n                        SET updated_at = NOW(), last_event_sequence = $2\n                        WHERE auth_request_id = $1\n                    \"\"\", auth_request_id, next_seq)\n                \n                # Don't delete - let visibility timeout expire for retry\n    \n    finally:\n        # Always release lock\n        await release_lock(auth_request_id)\n```\n\n## Behaviors\n\n### B1: Exactly-Once Processing\n**Given** an auth request is queued\n**When** multiple workers attempt to process\n**Then** only ONE worker acquires the lock\n**And** only ONE processor call is made\n**And** lock prevents duplicate attempts\n\n### B2: Atomic Event + Read Model Update\n**Given** worker processes auth successfully\n**When** writing result\n**Then** event AND read model are written in same transaction\n**And** both commit or both rollback (no partial state)\n**And** read model immediately reflects latest event\n\n### B3: Void Race Condition Handling\n**Given** void is requested while auth is being processed\n**When** worker checks for void event before calling processor\n**Then** if void event exists, skip processing\n**And** atomically write AuthRequestExpired event + update read model to EXPIRED\n\n### B4: Transient Failure Retry\n**Given** processor returns 500 or timeout\n**When** retry count < MAX_RETRIES (5)\n**Then** record AuthAttemptFailed event with is_retryable=true\n**And** status remains PROCESSING (not FAILED yet)\n**And** message becomes visible again after visibility timeout\n**And** retry with exponential backoff\n\n### B5: Terminal Failure - Invalid Token\n**Given** Payment Token Service returns 404 (token not found)\n**When** worker processes\n**Then** atomically write AuthAttemptFailed event + update read model to FAILED\n**And** send to dead letter queue\n**And** do NOT retry\n\n### B6: Terminal Failure - Expired Token\n**Given** Payment Token Service returns 410 (token expired)\n**When** worker processes\n**Then** atomically write AuthAttemptFailed event + update read model to FAILED\n**And** send to dead letter queue\n**And** do NOT retry\n\n### B7: Terminal Failure - Max Retries\n**Given** processor fails 5 times\n**When** retry count = MAX_RETRIES\n**Then** atomically write final AuthAttemptFailed event + update read model to FAILED\n**And** send to dead letter queue\n\n### B8: Processor Denial (Not a Failure)\n**Given** processor returns decline (e.g., insufficient funds)\n**When** worker processes\n**Then** atomically write AuthResponseReceived (status=DENIED) + update read model to DENIED\n**And** include denial_code and denial_reason in read model\n**And** this is NOT a failure (expected outcome)\n**And** mark as completed\n\n### B9: Lock Timeout (Worker Crash)\n**Given** worker crashes while holding lock\n**When** lock TTL expires (30 seconds)\n**Then** another worker can acquire lock\n**And** reprocess the request\n**And** read model tracks retry count via events\n\n### B10: Config-Based Processor Routing\n**Given** restaurant config specifies processor=\"stripe\"\n**When** worker processes auth request\n**Then** use Stripe client\n**When** config changes to processor=\"chase\"\n**Then** new requests use Chase client\n**And** replayed requests use current config\n\n### B11: Payment Token Service Unavailable\n**Given** Payment Token Service is down or timing out\n**When** worker attempts to decrypt token\n**Then** treat as retryable error (like processor timeout)\n**And** retry with backoff\n**And** after MAX_RETRIES, atomically write event + update to FAILED + send to DLQ\n\n## Payment Token Service Client\n\n```python\nclass PaymentTokenServiceClient:\n    def __init__(self, base_url: str, service_auth_token: str):\n        self.base_url = base_url\n        self.auth_token = service_auth_token\n    \n    async def decrypt(\n        self,\n        payment_token: str,\n        restaurant_id: str,\n        requesting_service: str\n    ) -> PaymentData:\n        \"\"\"\n        Calls Payment Token Service /internal/decrypt endpoint.\n        \n        Returns:\n            PaymentData with decrypted card details\n        \n        Raises:\n            TokenNotFound: 404 - token doesn't exist\n            TokenExpired: 410 - token expired\n            Forbidden: 403 - restaurant mismatch or unauthorized\n            ServiceUnavailable: 5xx or timeout\n        \"\"\"\n        response = await self.http_client.post(\n            f\"{self.base_url}/internal/v1/decrypt\",\n            headers={\n                \"Content-Type\": \"application/x-protobuf\",\n                \"X-Service-Auth\": self.auth_token,\n                \"X-Request-ID\": generate_correlation_id()\n            },\n            data=DecryptTokenRequest(\n                payment_token=payment_token,\n                restaurant_id=restaurant_id,\n                requesting_service=requesting_service\n            ).SerializeToString(),\n            timeout=5.0\n        )\n        \n        if response.status == 404:\n            raise TokenNotFound(f\"Token {payment_token} not found\")\n        elif response.status == 410:\n            raise TokenExpired(f\"Token {payment_token} expired\")\n        elif response.status == 403:\n            raise Forbidden(f\"Unauthorized access to token {payment_token}\")\n        elif response.status >= 500 or response.timeout:\n            raise ServiceUnavailable(\"Payment Token Service unavailable\")\n        \n        return DecryptTokenResponse.FromString(response.body).payment_data\n```\n\n## Processor Integration Interfaces\n\n### Stripe Processor\n```python\nclass StripeProcessor:\n    async def authorize(\n        self,\n        payment_data: PaymentData,\n        amount_cents: int,\n        currency: str,\n        config: dict\n    ) -> AuthorizationResult:\n        \"\"\"\n        Calls Stripe API: POST /v1/charges\n        \n        Errors:\n        - StripeCardError (decline) → DENIED\n        - StripeAPIError (5xx) → Retry\n        - StripeTimeout → Retry\n        \"\"\"\n        import stripe\n        \n        stripe.api_key = config['api_key']\n        \n        try:\n            charge = stripe.Charge.create(\n                amount=amount_cents,\n                currency=currency.lower(),\n                source={\n                    \"object\": \"card\",\n                    \"number\": payment_data.card_number,\n                    \"exp_month\": payment_data.exp_month,\n                    \"exp_year\": payment_data.exp_year,\n                    \"cvc\": payment_data.cvv,\n                    \"name\": payment_data.cardholder_name\n                },\n                capture=False,  # Authorization only, capture later\n                statement_descriptor=config.get('statement_descriptor')\n            )\n            \n            return AuthorizationResult(\n                status=AuthStatus.AUTHORIZED,\n                processor_auth_id=charge.id,\n                processor_name=\"stripe\",\n                authorized_amount_cents=charge.amount,\n                currency=charge.currency.upper(),\n                authorization_code=charge.authorization_code,\n                authorized_at=charge.created,\n                processor_metadata={\n                    \"balance_transaction\": charge.balance_transaction,\n                    \"network\": charge.payment_method_details.card.network\n                }\n            )\n            \n        except stripe.error.CardError as e:\n            # Card declined\n            return AuthorizationResult(\n                status=AuthStatus.DENIED,\n                processor_name=\"stripe\",\n                denial_code=e.code,\n                denial_reason=e.user_message,\n                processor_metadata={\"decline_code\": e.decline_code}\n            )\n        \n        except (stripe.error.APIError, stripe.error.RateLimitError) as e:\n            # Retryable errors\n            raise ProcessorTimeout(f\"Stripe API error: {e}\")\n```\n\n### Chase Processor (Future)\n```python\nclass ChaseProcessor:\n    async def authorize(...) -> AuthorizationResult:\n        \"\"\"\n        Calls Chase payment gateway\n        \"\"\"\n```\n\n## Error Classification\n\n**Retryable Errors (Transient):**\n- Processor timeout\n- Processor 500/503 errors\n- Network errors\n- Payment Token Service unavailable (5xx, timeout)\n\n**Non-Retryable Errors (Terminal):**\n- Invalid payment token (404 from Payment Token Service)\n- Expired payment token (410 from Payment Token Service)\n- Invalid request format (400)\n- Authentication failure (401)\n- Processor hard decline (but record as DENIED, not FAILED)\n\n## Database Operations\n\n### Acquire Lock\n```sql\nINSERT INTO auth_processing_locks (auth_request_id, worker_id, expires_at)\nVALUES ($1, $2, NOW() + INTERVAL '30 seconds')\nON CONFLICT (auth_request_id) DO NOTHING\nRETURNING auth_request_id;\n\n-- If returns NULL → lock already held\n```\n\n### Release Lock\n```sql\nDELETE FROM auth_processing_locks\nWHERE auth_request_id = $1 AND worker_id = $2;\n```\n\n### Check for Void\n```sql\nSELECT EXISTS (\n    SELECT 1 FROM payment_events\n    WHERE aggregate_id = $1\n      AND event_type = 'AuthVoidRequested'\n) AS is_voided;\n```\n\n### Get Next Sequence Number\n```sql\nSELECT COALESCE(MAX(sequence_number), 0) + 1\nFROM payment_events\nWHERE aggregate_id = $1;\n```\n\n### Cleanup Expired Locks (Background Task)\n```sql\nDELETE FROM auth_processing_locks\nWHERE expires_at < NOW();\n```\n\n## Configuration\n\n```yaml\nworker:\n  sqs_queue_url: \"https://sqs.us-east-1.amazonaws.com/.../auth-requests.fifo\"\n  batch_size: 1\n  wait_time_seconds: 20  # Long polling\n  visibility_timeout: 30\n  max_retries: 5\n  lock_ttl_seconds: 30\n  \npayment_token_service:\n  base_url: \"https://payment-token-service.internal\"\n  service_auth_token: \"<from-secrets-manager>\"\n  timeout_seconds: 5\n  max_retries: 2\n  \nprocessors:\n  stripe:\n    api_key: \"sk_live_...\"\n    timeout_seconds: 10\n  chase:\n    merchant_id: \"...\"\n    timeout_seconds: 15\n```\n\n## Dependencies\n- **Payment Token Service**: Internal /internal/decrypt endpoint\n- **Payment Processors**: Stripe, Chase, Worldpay APIs\n- **PostgreSQL**: Event store and read models (shared with Authorization API)\n- **SQS**: Auth request queue\n\n## Deployment\n- **ECS Service**: Long-running workers (recommended for stable connections)\n- **OR Lambda**: Triggered by SQS (for auto-scaling, but cold starts)\n- **Scaling**: Auto-scale based on SQS queue depth\n- **Health Check**: Worker heartbeat to CloudWatch\n\n## Monitoring & Observability\n- **Metrics**: Auth processing latency, success/failure rates, retry counts, lock contention\n- **Logs**: Structured logging (JSON) with correlation IDs\n- **Alarms**: \n  - Dead letter queue depth > 10\n  - Processing latency > p99\n  - Payment Token Service error rate > 5%\n- **Tracing**: X-Ray for distributed tracing across services\n\n## Testing Strategy\n- **Unit tests**: Lock acquisition, error classification, retry logic, transaction logic\n- **Integration tests**: Full flow with mocked Payment Token Service + mocked processor + real database\n- **Contract tests**: Verify Payment Token Service client matches actual API\n- **Transaction tests**: Verify event + read model are atomic (simulate failures)\n- **Chaos tests**: Simulate worker crashes, processor timeouts, lock expiry, Payment Token Service outages\n- **Load tests**: Sustained 300 QPS with queue backlog","priority":0,"archived":0,"archived_at":null,"created_at":"2025-11-10 04:35:31","updated_at":"2025-11-10 06:42:17","parent_id":null,"parent_uuid":null,"relationships":[{"from":"s-w5sf","from_type":"spec","to":"s-7ujm","to_type":"spec","type":"depends-on"},{"from":"s-w5sf","from_type":"spec","to":"s-94si","to_type":"spec","type":"implements"},{"from":"s-w5sf","from_type":"spec","to":"s-8c0t","to_type":"spec","type":"depends-on"}],"tags":["processor","service-spec","worker"]}
{"id":"s-4nmw","uuid":"4eaf2f58-1686-4093-b8d2-cae3dd56021f","title":"Detokenization Service","file_path":"specs/detokenization_service.md","content":"## DEPRECATED\n\nThis service has been merged into the **Payment Token Service** (s-7ujm).\n\nThe internal decryption endpoint `/internal/decrypt` is now part of the Payment Token Service, as both services share the same database and PCI compliance boundary.\n\nSee **Payment Token Service** spec for the combined service architecture.\n\n## Migration Notes\n- Auth Processor Workers now call Payment Token Service directly\n- Detokenization logic is part of Payment Token Service\n- Single database, single service for PCI zone\n- Internal `/internal/decrypt` endpoint documented in Payment Token Service spec\n","priority":0,"archived":1,"archived_at":"2025-11-10T06:47:51.946Z","created_at":"2025-11-10 04:35:32","updated_at":"2025-11-10 06:47:51","parent_id":null,"parent_uuid":null,"relationships":[],"tags":["internal","pci-compliant","service-spec"]}
{"id":"s-8c0t","uuid":"c9d1a884-9641-4933-ab43-1161a52be508","title":"Shared Infrastructure Components","file_path":"specs/shared_infrastructure_components.md","content":"## Overview\n\nShared infrastructure, database schemas, queue definitions, and cross-cutting concerns used by all services. Includes **Transactional Outbox Pattern** for reliable event-to-queue delivery.\n\n## Database: PostgreSQL (Aurora)\n\n### Instance Configuration\n\n- **Engine**: PostgreSQL 15+ (Aurora Serverless v2 recommended)\n- **Multi-AZ**: Enabled for high availability\n- **Backups**: Automated daily snapshots, 30-day retention\n- **Read Replicas**: 1-2 for read scaling (status queries)\n- **Connection Pooling**: PgBouncer or RDS Proxy\n\n### Database Separation\n\n**Payment Token Database** (PCI zone):\n\n- Isolated RDS instance\n- Only accessible by Payment Token Service\n- Encrypted at rest with AWS KMS LALALA\n- Separate VPC subnet with strict security groups\n\n**Payment Events Database** (main):\n\n- Shared by Authorization API and Auth Processor Workers\n- Contains: payment\\_events, auth\\_request\\_state, outbox, configs\n\n```sql\n-- payment_tokens_db (isolated)\nCREATE DATABASE payment_tokens_db;\n\n-- payment_events_db (main)\nCREATE DATABASE payment_events_db;\n```\n\n## Queue Architecture (AWS SQS)\n\n### Auth Request Queue (FIFO)\n\n```\nName: payment-auth-requests.fifo\nType: FIFO\nDeduplication: Message deduplication ID = auth_request_id\nMessage Group ID: restaurant_id\nVisibility Timeout: 30 seconds\nMessage Retention: 4 days\nMax Receive Count: 5\nDead Letter Queue: payment-auth-requests-dlq.fifo\n```\n\n**Message Format:**\n\n```json\n{\n  \"auth_request_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"restaurant_id\": \"f47ac10b-58cc-4372-a567-0e02b2c3d479\",\n  \"created_at\": 1234567890\n}\n```\n\n**Population Method**: Outbox processor (runs in Authorization API) polls `outbox` table and sends to SQS\n\n### Void Request Queue (Standard)\n\n```\nName: payment-void-requests\nType: Standard (order not critical)\nVisibility Timeout: 60 seconds\nMessage Retention: 14 days (retry for up to X hours)\nMax Receive Count: 20 (more retries for voids)\nDead Letter Queue: payment-void-requests-dlq\n```\n\n### Dead Letter Queues\n\n**Auth Requests DLQ:**\n\n```\nName: payment-auth-requests-dlq.fifo\nPurpose: Terminal auth failures (invalid token, max retries)\nAlarm: > 10 messages\nRetention: 14 days (manual review)\n```\n\n**Void Requests DLQ:**\n\n```\nName: payment-void-requests-dlq\nPurpose: Failed voids after all retries\nAlarm: > 5 messages\n```\n\n## Event Store Schema (Detailed)\n\n```sql\n-- Main event store (append-only, never update/delete)\nCREATE TABLE payment_events (\n    id BIGSERIAL PRIMARY KEY,\n    event_id UUID UNIQUE NOT NULL DEFAULT gen_random_uuid(),\n    aggregate_id UUID NOT NULL,  -- auth_request_id, void_request_id, etc.\n    aggregate_type VARCHAR(50) NOT NULL,  -- 'auth_request', 'void_request'\n    event_type VARCHAR(100) NOT NULL,\n    event_data BYTEA NOT NULL,  -- Protobuf serialized\n    metadata JSONB DEFAULT '{}'::jsonb,\n    created_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    sequence_number INTEGER NOT NULL,\n\n    CONSTRAINT unique_aggregate_sequence UNIQUE(aggregate_id, sequence_number),\n    CONSTRAINT check_sequence_positive CHECK (sequence_number > 0)\n);\n\n-- Indexes for fast queries\nCREATE INDEX idx_aggregate_events ON payment_events(aggregate_id, sequence_number);\nCREATE INDEX idx_event_type_created ON payment_events(event_type, created_at DESC);\nCREATE INDEX idx_created_at ON payment_events(created_at DESC);\n\n-- Partitioning for archival (optional, for future)\n-- Partition by created_at monthly for easy archival\n```\n\n## Transactional Outbox Pattern\n\n### Outbox Table\n\n**Critical for reliable event → queue delivery with at-least-once guarantee.**\n\n```sql\n-- Outbox for reliable queue delivery (Transactional Outbox Pattern)\nCREATE TABLE outbox (\n    id BIGSERIAL PRIMARY KEY,\n    aggregate_id UUID NOT NULL,  -- auth_request_id or void_request_id\n    message_type VARCHAR(100) NOT NULL,  -- 'auth_request_queued', 'void_request_queued'\n    payload JSONB NOT NULL,  -- Message body for SQS\n    created_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    processed_at TIMESTAMP,  -- NULL = unprocessed, NOT NULL = sent to SQS\n\n    INDEX idx_unprocessed (created_at) WHERE processed_at IS NULL  -- Partial index for efficiency!\n);\n```\n\n**How it works:**\n\n1. Authorization API writes event + read model + outbox entry in **single transaction**\n1. Outbox processor (background thread in Authorization API) polls `WHERE processed_at IS NULL`\n1. Sends messages to SQS\n1. Marks as processed: `UPDATE outbox SET processed_at = NOW()`\n\n**Guarantees:**\n\n- ✅ Atomic write (event + outbox)\n- ✅ At-least-once delivery to SQS\n- ✅ No message loss (survives crashes)\n- ✅ SQS FIFO deduplication handles duplicates\n\n### Outbox Cleanup (Optional)\n\nProcessed outbox entries can be archived/deleted after retention period:\n\n```sql\n-- Delete processed outbox entries older than 7 days\nDELETE FROM outbox\nWHERE processed_at < NOW() - INTERVAL '7 days';\n```\n\nRun daily as maintenance job.\n\n## Read Models (Materialized Views)\n\n### Auth Request State (Optimized for Queries)\n\n**Updated synchronously in same transaction as events (by Authorization API and Workers).**\n\n```sql\nCREATE TABLE auth_request_state (\n    auth_request_id UUID PRIMARY KEY,\n    restaurant_id UUID NOT NULL,\n    payment_token VARCHAR(64) NOT NULL,\n\n    -- Current state\n    status VARCHAR(20) NOT NULL,  -- PENDING, PROCESSING, AUTHORIZED, DENIED, etc.\n\n    -- Request details\n    amount_cents BIGINT NOT NULL,\n    currency VARCHAR(3) NOT NULL,\n\n    -- Result (populated when completed by worker)\n    processor_auth_id VARCHAR(255),\n    processor_name VARCHAR(50),\n    authorized_amount_cents BIGINT,\n    authorization_code VARCHAR(100),\n\n    -- Denial details (if DENIED)\n    denial_code VARCHAR(50),\n    denial_reason TEXT,\n\n    -- Timestamps\n    created_at TIMESTAMP NOT NULL,\n    updated_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    completed_at TIMESTAMP,\n\n    -- Metadata\n    metadata JSONB DEFAULT '{}'::jsonb,\n\n    -- Event sourcing bookkeeping\n    last_event_sequence INTEGER NOT NULL DEFAULT 0,\n    last_event_id UUID,\n\n    -- Indexes\n    CONSTRAINT check_status CHECK (status IN ('PENDING', 'PROCESSING', 'AUTHORIZED', 'DENIED', 'FAILED', 'VOIDED', 'EXPIRED'))\n);\n\nCREATE INDEX idx_restaurant_created ON auth_request_state(restaurant_id, created_at DESC);\nCREATE INDEX idx_status ON auth_request_state(status) WHERE status IN ('PENDING', 'PROCESSING');\nCREATE INDEX idx_payment_token ON auth_request_state(payment_token);\nCREATE INDEX idx_completed_at ON auth_request_state(completed_at DESC) WHERE completed_at IS NOT NULL;\n\n-- Auto-update updated_at trigger\nCREATE OR REPLACE FUNCTION update_updated_at_column()\nRETURNS TRIGGER AS $$\nBEGIN\n   NEW.updated_at = NOW();\n   RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER update_auth_request_state_updated_at\n    BEFORE UPDATE ON auth_request_state\n    FOR EACH ROW\n    EXECUTE FUNCTION update_updated_at_column();\n```\n\n### Restaurant Payment Configs (Cache)\n\n```sql\nCREATE TABLE restaurant_payment_configs (\n    restaurant_id UUID PRIMARY KEY,\n    config_version VARCHAR(50) NOT NULL,\n\n    -- Processor selection\n    processor_name VARCHAR(50) NOT NULL,  -- \"stripe\", \"chase\", \"worldpay\"\n\n    -- Processor-specific config (JSON)\n    processor_config JSONB NOT NULL,\n    -- Example for Stripe:\n    -- {\n    --   \"stripe_api_key\": \"sk_test_...\",\n    --   \"statement_descriptor\": \"RESTAURANT NAME\"\n    -- }\n\n    -- Metadata\n    is_active BOOLEAN NOT NULL DEFAULT true,\n    updated_at TIMESTAMP NOT NULL DEFAULT NOW(),\n\n    CONSTRAINT check_processor CHECK (processor_name IN ('stripe', 'chase', 'worldpay'))\n);\n\nCREATE INDEX idx_active_configs ON restaurant_payment_configs(is_active) WHERE is_active = true;\n\n-- Initial seed data (for testing - single restaurant with Stripe)\nINSERT INTO restaurant_payment_configs (restaurant_id, config_version, processor_name, processor_config, updated_at)\nVALUES (\n    '00000000-0000-0000-0000-000000000001'::UUID,\n    'v1',\n    'stripe',\n    '{\"stripe_api_key\": \"sk_test_...\", \"statement_descriptor\": \"TEST RESTAURANT\"}'::JSONB,\n    NOW()\n);\n```\n\n### Idempotency Keys\n\n```sql\nCREATE TABLE auth_idempotency_keys (\n    idempotency_key VARCHAR(255) NOT NULL,\n    restaurant_id UUID NOT NULL,\n    auth_request_id UUID NOT NULL,\n    created_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    expires_at TIMESTAMP NOT NULL DEFAULT NOW() + INTERVAL '24 hours',\n\n    PRIMARY KEY (idempotency_key, restaurant_id)\n);\n\nCREATE INDEX idx_idempotency_expires ON auth_idempotency_keys(expires_at);\n\n-- Cleanup expired keys (background job)\nCREATE INDEX idx_expired_keys ON auth_idempotency_keys(expires_at) WHERE expires_at < NOW();\n```\n\n### Processing Locks\n\n```sql\nCREATE TABLE auth_processing_locks (\n    auth_request_id UUID PRIMARY KEY,\n    worker_id VARCHAR(255) NOT NULL,\n    locked_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    expires_at TIMESTAMP NOT NULL DEFAULT NOW() + INTERVAL '30 seconds',\n\n    CONSTRAINT check_expires_after_locked CHECK (expires_at > locked_at)\n);\n\nCREATE INDEX idx_lock_expires ON auth_processing_locks(expires_at);\n\n-- Cleanup expired locks (background job)\n-- Note: This is also done by workers, but good hygiene to have separate cleanup\n```\n\n## Protobuf Definitions (Shared)\n\nAll services share common protobuf definitions for consistency.\n\n**See Issue [[i-xxxx]]: Complete Protobuf Definitions** for full `.proto` files.\n\n```protobuf\nsyntax = \"proto3\";\n\npackage payments.v1;\n\n// Common types\nmessage Money {\n  int64 amount_cents = 1;\n  string currency = 2;  // ISO 4217\n}\n\nmessage Timestamp {\n  int64 seconds = 1;\n  int32 nanos = 2;\n}\n\n// Status enums\nenum AuthStatus {\n  AUTH_STATUS_UNSPECIFIED = 0;\n  AUTH_STATUS_PENDING = 1;\n  AUTH_STATUS_PROCESSING = 2;\n  AUTH_STATUS_AUTHORIZED = 3;\n  AUTH_STATUS_DENIED = 4;\n  AUTH_STATUS_FAILED = 5;\n  AUTH_STATUS_VOIDED = 6;\n  AUTH_STATUS_EXPIRED = 7;\n}\n\n// Event base\nmessage EventMetadata {\n  string event_id = 1;\n  string correlation_id = 2;\n  string causation_id = 3;\n  Timestamp created_at = 4;\n}\n\n// Error types\nmessage ErrorDetails {\n  string error_code = 1;\n  string error_message = 2;\n  bool is_retryable = 3;\n  int32 retry_count = 4;\n}\n```\n\n## Archival Strategy\n\n### Hot Data (0-30 days)\n\n- Stored in primary PostgreSQL database\n- Full query capabilities\n- Indexed for fast lookups\n\n### Warm Data (30-365 days)\n\n- Move to separate archive table or database\n- Partitioned by month\n- Less frequently accessed\n- Consider read replicas for queries\n\n### Cold Data (365+ days)\n\n- Export to S3 (Parquet format)\n- Queryable via AWS Athena\n- Compressed and encrypted\n- Retained for 7 years (PCI compliance)\n\n### Archival Process (Cron Job)\n\n```sql\n-- Monthly job: Archive events older than 30 days\nINSERT INTO payment_events_archive\nSELECT * FROM payment_events\nWHERE created_at < NOW() - INTERVAL '30 days';\n\nDELETE FROM payment_events\nWHERE created_at < NOW() - INTERVAL '30 days';\n\n-- Export to S3\nCOPY payment_events_archive TO 's3://payment-events/archive/2024-01.parquet'\nWITH (FORMAT PARQUET, COMPRESSION GZIP);\n```\n\n## Monitoring & Observability\n\n### Key Metrics\n\n- **Auth Request Latency**: p50, p95, p99 from POST /authorize to completion\n- **Queue Depth**: Number of pending messages in SQS\n- **Outbox Depth**: Number of unprocessed outbox entries (`COUNT(*) WHERE processed_at IS NULL`)\n- **Worker Processing Rate**: Messages processed per second\n- **Error Rates**: Failed auths, DLQ depth\n- **Lock Contention**: Failed lock acquisitions\n\n### CloudWatch Alarms\n\n- **DLQ depth > 10** → Page on-call\n- **Outbox unprocessed > 1000** → Warning (outbox processor may be down)\n- **Auth latency p99 > 10s** → Warning\n- **Worker processing rate drop > 50%** → Warning\n- **Database CPU > 80%** → Scale up\n\n### Structured Logging Format\n\n```json\n{\n  \"timestamp\": \"2024-01-15T10:30:00Z\",\n  \"level\": \"INFO\",\n  \"service\": \"auth-processor-worker\",\n  \"correlation_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"auth_request_id\": \"f47ac10b-58cc-4372-a567-0e02b2c3d479\",\n  \"event\": \"auth_attempt_started\",\n  \"duration_ms\": 150,\n  \"processor\": \"stripe\"\n}\n```\n\n## Security & Compliance\n\n### PCI DSS Requirements\n\n- Payment Token Service isolated (separate VPC, database)\n- All payment data encrypted at rest (AWS KMS)\n- All connections use TLS 1.3\n- Audit logs immutable and retained\n- No payment data in application logs\n\n### Network Architecture\n\n```\n┌─────────────────────────────────────────────────────┐\n│                  Public Subnet                      │\n│  - API Gateway / Load Balancer                      │\n└─────────────────────────────────────────────────────┘\n                      │\n┌─────────────────────────────────────────────────────┐\n│                 Private Subnet (App)                │\n│  - Authorization API (with outbox processor)        │\n│  - Auth Processor Workers                           │\n└─────────────────────────────────────────────────────┘\n                      │\n┌─────────────────────────────────────────────────────┐\n│             Private Subnet (PCI Zone)               │\n│  - Payment Token Service                            │\n│  - Payment Token Database (isolated RDS)            │\n└─────────────────────────────────────────────────────┘\n                      │\n┌─────────────────────────────────────────────────────┐\n│                 Private Subnet (Data)               │\n│  - Payment Events Database (RDS Aurora)             │\n│    - payment_events                                 │\n│    - auth_request_state                             │\n│    - outbox                                         │\n│  - ElastiCache (optional, for caching)              │\n└─────────────────────────────────────────────────────┘\n```\n\n## Testing Infrastructure\n\n### LocalStack (for local development)\n\n```yaml\n# docker-compose.yml\nservices:\n  localstack:\n    image: localstack/localstack\n    ports:\n      - \"4566:4566\"\n    environment:\n      - SERVICES=sqs,kms\n      - DEBUG=1\n\n  postgres:\n    image: postgres:15\n    ports:\n      - \"5432:5432\"\n    environment:\n      - POSTGRES_PASSWORD=password\n      - POSTGRES_DB=payment_events_db\n    volumes:\n      - ./init.sql:/docker-entrypoint-initdb.d/init.sql\n```\n\n### init.sql (Database Bootstrap)\n\n```sql\n-- Run all CREATE TABLE statements from above\n-- Includes: payment_events, outbox, auth_request_state, etc.\n```\n\n### Test Data Fixtures\n\n- Factory pattern for creating test events\n- Seed data for restaurant configs\n- Mock payment processor responses\n\n## Configuration Management\n\nAll configuration stored in AWS SSM Parameter Store or Secrets Manager:\n\n```\n/payments/production/database/url\n/payments/production/sqs/auth-requests-queue-url\n/payments/production/stripe/api-key (SecretString)\n/payments/production/token-service/internal-api-url\n```\n\n## Background Jobs\n\n### Outbox Processor (in Authorization API)\n\n- **Frequency**: Every 100ms\n- **Query**: `SELECT * FROM outbox WHERE processed_at IS NULL ORDER BY created_at LIMIT 100 FOR UPDATE SKIP LOCKED`\n- **Action**: Send to SQS, mark as processed\n\n### Expired Idempotency Keys Cleanup\n\n- **Frequency**: Hourly\n- **Query**: `DELETE FROM auth_idempotency_keys WHERE expires_at < NOW()`\n\n### Expired Locks Cleanup\n\n- **Frequency**: Every minute\n- **Query**: `DELETE FROM auth_processing_locks WHERE expires_at < NOW()`\n\n### Outbox Cleanup (Optional)\n\n- **Frequency**: Daily\n- **Query**: `DELETE FROM outbox WHERE processed_at < NOW() - INTERVAL '7 days'`","priority":0,"archived":0,"archived_at":null,"created_at":"2025-11-10 04:35:33","updated_at":"2025-11-10 18:14:47","parent_id":null,"parent_uuid":null,"relationships":[],"tags":["database","infrastructure","shared"]}
{"id":"s-94si","uuid":"a5b75599-960a-478a-9832-126b098c2faf","title":"Event Sourcing & Read Model Architecture","file_path":"specs/event_sourcing_read_model_architecture.md","content":"## Overview\nComprehensive guide to the event sourcing and read model architecture used across the payments infrastructure. This document clarifies **transaction boundaries**, **read model update patterns**, and **outbox-based queue integration**.\n\n## Core Principles\n\n1. **Events are immutable source of truth** - stored in `payment_events` table, never updated or deleted\n2. **Read models are derived projections** - updated synchronously in same transaction as events\n3. **Outbox pattern ensures reliable queue delivery** - atomic writes, at-least-once delivery\n4. **Strong consistency for reads** - no eventual consistency delay for status queries\n\n## Transaction Boundaries\n\n### Authorization API: POST /authorize\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                     BEGIN TRANSACTION                        │\n│                                                              │\n│  1. Check idempotency (SELECT)                               │\n│  2. Write event: AuthRequestCreated                          │\n│     INSERT INTO payment_events (...)                         │\n│                                                              │\n│  3. Write read model                                         │\n│     INSERT INTO auth_request_state (status='PENDING', ...)   │\n│                                                              │\n│  4. Write outbox entry                                       │\n│     INSERT INTO outbox (message_type='auth_request_queued')  │\n│                                                              │\n│  5. Write idempotency key                                    │\n│     INSERT INTO auth_idempotency_keys (...)                  │\n│                                                              │\n│                      COMMIT                                  │\n└─────────────────────────────────────────────────────────────┘\n\nResult: Event + Read Model + Outbox + Idempotency written atomically\nIf any step fails → entire transaction rolls back\n```\n\n### Auth Processor Worker: Process Request\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│  1. Dequeue from SQS (visibility timeout starts)             │\n│  2. Acquire lock (INSERT INTO auth_processing_locks)         │\n│  3. Check for void event (SELECT)                            │\n│  4. Fetch config (SELECT)                                    │\n│  5. Call Payment Token Service (external API)                │\n│  6. Call Payment Processor (external API - Stripe)           │\n│                                                              │\n│                     BEGIN TRANSACTION                        │\n│                                                              │\n│  7. Write event: AuthResponseReceived                        │\n│     INSERT INTO payment_events (...)                         │\n│                                                              │\n│  8. Update read model                                        │\n│     UPDATE auth_request_state                                │\n│     SET status='AUTHORIZED', processor_auth_id=..., ...      │\n│                                                              │\n│                      COMMIT                                  │\n│                                                              │\n│  9. Delete SQS message (ACK)                                 │\n│  10. Release lock (DELETE FROM auth_processing_locks)        │\n└─────────────────────────────────────────────────────────────┘\n\nResult: Event + Read Model updated atomically\nExternal API calls happen BEFORE transaction (if they fail, retry via SQS)\n```\n\n## Read Model Update Patterns\n\n### Pattern: Synchronous Updates (RECOMMENDED)\n\n**Both Authorization API and Workers update read models in same transaction as events.**\n\n**Advantages:**\n- ✅ Strong consistency - read models always reflect latest events\n- ✅ No delay for status queries\n- ✅ Simpler architecture - no separate projection service\n- ✅ Transactional guarantees\n\n**Trade-offs:**\n- ⚠️ Read model logic distributed across services (API writes PENDING, worker writes AUTHORIZED)\n- ⚠️ Cannot rebuild read models from events alone (need application logic)\n\n**When to use:** When you need immediate consistency for user-facing queries (like payment status).\n\n### Alternative Pattern: Event-Driven Projections (NOT USED)\n\nA separate projection service listens to events and updates read models asynchronously.\n\n**NOT recommended for this system** because:\n- ❌ Eventual consistency not acceptable for payment status (users expect immediate updates)\n- ❌ Adds complexity (another service to maintain)\n- ❌ 100-500ms delay unacceptable for POST /authorize 5-second polling\n\n**When to use:** Analytics, reporting, or non-critical read models where eventual consistency is acceptable.\n\n## Outbox Pattern Details\n\n### Purpose\n\nEnsure **atomic** write of events + queue messages without distributed transactions.\n\n### How It Works\n\n```\nAuthorization API Transaction:\n  1. Write event to payment_events\n  2. Write read model to auth_request_state\n  3. Write outbox entry to outbox table\n  ───────────────────────────────────────\n  COMMIT (all succeed or all fail)\n\nBackground Outbox Processor (every 100ms):\n  1. SELECT * FROM outbox WHERE processed_at IS NULL FOR UPDATE SKIP LOCKED\n  2. Send each message to SQS\n  3. UPDATE outbox SET processed_at = NOW()\n```\n\n### Guarantees\n\n| Scenario | Outcome |\n|----------|---------|\n| Transaction commits | Outbox entry exists, will be sent to SQS |\n| Transaction rolls back | No outbox entry, no SQS message |\n| Outbox processor crashes | Unprocessed messages remain in DB, retried on restart |\n| SQS send fails | Outbox entry remains unprocessed, retried on next poll |\n| Duplicate SQS sends | SQS FIFO deduplication prevents duplicate processing |\n\n### Failure Modes\n\n**Q: What if outbox processor is down for extended period?**  \nA: Outbox entries queue up in database. Alarms trigger at > 1000 unprocessed. Messages sent when processor restarts.\n\n**Q: What if database fails after SQS send but before marking as processed?**  \nA: Message sent twice. SQS FIFO deduplication (MessageDeduplicationId=auth_request_id) prevents duplicate processing by worker.\n\n**Q: What if outbox table fills up?**  \nA: Run daily cleanup job: `DELETE FROM outbox WHERE processed_at < NOW() - INTERVAL '7 days'`. Monitor disk usage.\n\n## Event Replay & Debugging\n\n### Replaying Events\n\nEvents are immutable, so you can replay them to rebuild state:\n\n```sql\n-- Rebuild auth_request_state for a specific request\nDELETE FROM auth_request_state WHERE auth_request_id = $1;\n\nSELECT aggregate_id, event_type, event_data, sequence_number\nFROM payment_events\nWHERE aggregate_id = $1\nORDER BY sequence_number;\n\n-- Apply each event in order to reconstruct state\n```\n\n### Debugging Payment Flow\n\n```sql\n-- See full history of an auth request\nSELECT \n    event_type,\n    event_data,\n    created_at,\n    sequence_number\nFROM payment_events\nWHERE aggregate_id = '550e8400-e29b-41d4-a716-446655440000'\nORDER BY sequence_number;\n\n-- Check current state\nSELECT * FROM auth_request_state\nWHERE auth_request_id = '550e8400-e29b-41d4-a716-446655440000';\n\n-- Check if queued\nSELECT * FROM outbox\nWHERE aggregate_id = '550e8400-e29b-41d4-a716-446655440000';\n```\n\n## Consistency Checks\n\n### Event vs Read Model Consistency\n\nVerify read model is up-to-date:\n\n```sql\n-- Check if read model is behind\nSELECT \n    ars.auth_request_id,\n    ars.status,\n    ars.last_event_sequence,\n    (SELECT MAX(sequence_number) FROM payment_events WHERE aggregate_id = ars.auth_request_id) AS latest_event_sequence\nFROM auth_request_state ars\nWHERE ars.last_event_sequence < (SELECT MAX(sequence_number) FROM payment_events WHERE aggregate_id = ars.auth_request_id);\n```\n\nIf rows returned → read model is stale (should never happen if transactions are used correctly).\n\n### Outbox Lag\n\nMonitor unprocessed outbox entries:\n\n```sql\n-- Check outbox depth\nSELECT COUNT(*) AS unprocessed_count\nFROM outbox\nWHERE processed_at IS NULL;\n\n-- Check oldest unprocessed message\nSELECT MIN(created_at) AS oldest_unprocessed\nFROM outbox\nWHERE processed_at IS NULL;\n```\n\nAlert if `unprocessed_count > 1000` or `oldest_unprocessed > 5 minutes`.\n\n## Idempotency\n\n### Request-Level Idempotency\n\nClient provides `X-Idempotency-Key` header:\n\n```\nPOST /authorize\nX-Idempotency-Key: 8f7d6c5b-4a3e-2d1c-0b9a-8e7f6d5c4b3a\n\n→ Same idempotency key within 24 hours returns same auth_request_id\n```\n\n**Implementation:**\n```sql\n-- Check idempotency\nSELECT auth_request_id FROM auth_idempotency_keys\nWHERE idempotency_key = $1 AND restaurant_id = $2;\n\n-- If exists, return existing auth_request\n-- If not exists, create new auth request + write idempotency key\n```\n\n### Event-Level Idempotency\n\nEach event has unique `event_id` (UUID):\n\n```sql\nINSERT INTO payment_events (event_id, aggregate_id, event_type, ...)\nVALUES ($1, $2, $3, ...)\nON CONFLICT (event_id) DO NOTHING;\n```\n\nPrevents duplicate events if retry occurs.\n\n### Outbox-Level Idempotency\n\nSQS FIFO deduplication:\n\n```python\nsqs.send_message(\n    QueueUrl=\"auth-requests.fifo\",\n    MessageBody=payload,\n    MessageDeduplicationId=auth_request_id,  # Deduplication key\n    MessageGroupId=restaurant_id\n)\n```\n\nIf same `auth_request_id` sent twice within 5 minutes → SQS ignores duplicate.\n\n## Sequence Diagrams\n\n### Happy Path: POST /authorize → Worker → Response\n\n```\nClient          Auth API         Outbox Proc    SQS Queue      Worker          Processor\n  │                │                 │              │             │                │\n  ├─POST /auth────>│                 │              │             │                │\n  │                ├─BEGIN TX────────┤              │             │                │\n  │                ├─Write Event─────┤              │             │                │\n  │                ├─Write Read Model│              │             │                │\n  │                ├─Write Outbox────┤              │             │                │\n  │                ├─COMMIT──────────┤              │             │                │\n  │                │                 │              │             │                │\n  │<─202 Accepted──┤ (returns immediately)          │             │                │\n  │                │                 │              │             │                │\n  │                │   (100ms later) │              │             │                │\n  │                │                 ├─Poll Outbox─>│             │                │\n  │                │                 ├─Send to SQS──>│            │                │\n  │                │                 ├─Mark Processed              │                │\n  │                │                 │              │             │                │\n  │                │                 │              ├─Dequeue────>│                │\n  │                │                 │              │             ├─Decrypt Token─>│\n  │                │                 │              │             ├─Authorize─────>│\n  │                │                 │              │             │<─Response──────┤\n  │                │                 │              │             │                │\n  │                │                 │              │             ├─BEGIN TX───────┤\n  │                │                 │              │             ├─Write Event────┤\n  │                │                 │              │             ├─Update Read Mdl┤\n  │                │                 │              │             ├─COMMIT─────────┤\n  │                │                 │              │             ├─Delete Msg─────┤\n  │                │                 │              │             │                │\n  ├─GET /status───>│                 │              │             │                │\n  │<─200 AUTHORIZED┤ (reads from read model)        │             │                │\n```\n\n### Error Path: Invalid Token\n\n```\nClient          Auth API         Outbox Proc    SQS Queue      Worker          Token Svc\n  │                │                 │              │             │                │\n  ├─POST /auth────>│                 │              │             │                │\n  │                ├─[Transaction: write event + read model + outbox]             │\n  │<─202 Accepted──┤                 │              │             │                │\n  │                │                 ├─[Outbox sends to SQS]─────>│                │\n  │                │                 │              ├─Dequeue────>│                │\n  │                │                 │              │             ├─Decrypt Token─>│\n  │                │                 │              │             │<─404 Not Found─┤\n  │                │                 │              │             │                │\n  │                │                 │              │             ├─BEGIN TX───────┤\n  │                │                 │              │             ├─Write Event────┤\n  │                │                 │              │             │  (AuthAttempt  │\n  │                │                 │              │             │   Failed)      │\n  │                │                 │              │             ├─Update Read Mdl┤\n  │                │                 │              │             │  (status=FAILED│\n  │                │                 │              │             ├─COMMIT─────────┤\n  │                │                 │              │             ├─Send to DLQ────┤\n  │                │                 │              │             ├─Delete Msg─────┤\n  │                │                 │              │             │                │\n  ├─GET /status───>│                 │              │             │                │\n  │<─200 FAILED────┤ (reads from read model)        │             │                │\n```\n\n## Best Practices\n\n### DO:\n- ✅ Always write events + read models in same transaction\n- ✅ Use outbox pattern for queue messages\n- ✅ Include sequence numbers on events for ordering\n- ✅ Store protobuf-serialized events for forward/backward compatibility\n- ✅ Monitor outbox depth and event lag\n\n### DON'T:\n- ❌ Never update or delete events\n- ❌ Don't send to SQS directly from application code (use outbox)\n- ❌ Don't update read models outside of transactions\n- ❌ Don't skip sequence numbers (use `MAX(sequence_number) + 1`)\n- ❌ Don't store sensitive data in event metadata\n\n## Testing Strategy\n\n### Unit Tests\n- Event serialization/deserialization\n- Read model projection logic\n- Idempotency key checks\n\n### Integration Tests\n- Full transaction flows (event + read model + outbox)\n- Outbox processor sends to LocalStack SQS\n- Worker processes and updates read model\n\n### Chaos Tests\n- Kill outbox processor mid-processing → verify messages retried\n- Simulate transaction failures → verify rollback\n- Send duplicate SQS messages → verify deduplication\n\n### Consistency Tests\n- Verify read model matches event history\n- Verify outbox eventually empties (no stuck messages)\n- Verify no lost messages (every event has outbox entry)\n\n## References\n\n- **Transactional Outbox Pattern**: https://microservices.io/patterns/data/transactional-outbox.html\n- **Event Sourcing**: https://martinfowler.com/eaaDev/EventSourcing.html\n- **CQRS**: https://martinfowler.com/bliki/CQRS.html","priority":0,"archived":0,"archived_at":null,"created_at":"2025-11-10 06:45:11","updated_at":"2025-11-10 06:45:11","parent_id":null,"parent_uuid":null,"relationships":[],"tags":["architecture","event-sourcing","patterns"]}
{"id":"s-5for","uuid":"b5cae9c7-0d03-4083-bb77-624518c8de67","title":"Payment Token Service - AWS Deployment Infrastructure","file_path":"specs/payment_token_service_aws_deployment_infrastructur.md","content":"## Overview\n\nTerraform infrastructure specification for deploying the Payment Token Service ([[s-7ujm]]) to AWS with PCI DSS compliance, network isolation, autoscaling, and high availability. This spec defines the complete AWS infrastructure stack required for production deployment.\n\n## Architecture Principles\n\n- **PCI Compliance**: Isolated VPC subnet, encrypted at rest/in transit, audit logging, no cardholder data in logs\n- **Zero Trust**: Network segmentation, mTLS for internal APIs, principle of least privilege\n- **High Availability**: Multi-AZ deployment, autoscaling, health checks, automatic failover\n- **Observability**: Comprehensive logging, metrics, tracing, and alerting\n- **Immutable Infrastructure**: Container-based deployments, blue-green strategy, infrastructure as code\n\n## AWS Services Used\n\n- **ECS Fargate**: Serverless container orchestration (no EC2 management)\n- **RDS PostgreSQL**: Isolated database instance for payment_tokens_db\n- **Application Load Balancer (ALB)**: Public API traffic distribution\n- **Network Load Balancer (NLB)**: Internal API for auth workers (VPC-only)\n- **API Gateway**: Public REST API with authentication, rate limiting, WAF\n- **KMS**: Base Derivation Key (BDK) and encryption key management\n- **Secrets Manager**: Database credentials, API keys, processor secrets\n- **CloudWatch**: Metrics, logs, alarms, dashboards\n- **VPC**: Network isolation with PCI-compliant subnet\n- **ECR**: Docker image registry\n- **Route 53**: DNS management\n- **Certificate Manager (ACM)**: TLS certificates\n\n## Network Architecture\n\n### VPC Design\n\n```\nVPC: payments-infra-vpc (10.0.0.0/16)\n├── Public Subnets (Internet-facing)\n│   ├── 10.0.1.0/24 (us-east-1a) - ALB, NAT Gateway\n│   └── 10.0.2.0/24 (us-east-1b) - ALB, NAT Gateway\n│\n├── Private Subnets - Application Tier\n│   ├── 10.0.10.0/24 (us-east-1a) - Authorization API, Auth Workers\n│   └── 10.0.11.0/24 (us-east-1b) - Authorization API, Auth Workers\n│\n├── Private Subnets - PCI Zone (ISOLATED)\n│   ├── 10.0.20.0/24 (us-east-1a) - Payment Token Service ECS Tasks\n│   └── 10.0.21.0/24 (us-east-1b) - Payment Token Service ECS Tasks\n│\n└── Private Subnets - Data Tier\n    ├── 10.0.30.0/24 (us-east-1a) - RDS payment_tokens_db, payment_events_db\n    └── 10.0.31.0/24 (us-east-1b) - RDS replicas\n```\n\n### Security Groups\n\n**SG-ALB-Public** (API Gateway VPC Link → Public ALB):\n```\nInbound:\n  - 443 from 0.0.0.0/0 (HTTPS only, via API Gateway VPC Link)\nOutbound:\n  - 8000 to SG-PTS-ECS (Payment Token Service)\n```\n\n**SG-NLB-Internal** (Auth Workers → Internal NLB):\n```\nInbound:\n  - 8001 from SG-Auth-Workers (internal /decrypt endpoint)\nOutbound:\n  - 8001 to SG-PTS-ECS\n```\n\n**SG-PTS-ECS** (Payment Token Service containers):\n```\nInbound:\n  - 8000 from SG-ALB-Public (public API)\n  - 8001 from SG-NLB-Internal (internal API)\nOutbound:\n  - 5432 to SG-RDS-PTS (PostgreSQL payment_tokens_db)\n  - 443 to AWS KMS endpoint (VPC endpoint)\n  - 443 to AWS Secrets Manager endpoint (VPC endpoint)\n  - 443 to CloudWatch Logs endpoint (VPC endpoint)\n```\n\n**SG-RDS-PTS** (Isolated RDS for payment_tokens_db):\n```\nInbound:\n  - 5432 from SG-PTS-ECS ONLY\nOutbound:\n  - None (database is destination only)\n```\n\n**SG-Auth-Workers** (Authorization processor workers):\n```\nOutbound:\n  - 8001 to SG-NLB-Internal (Payment Token Service internal API)\n```\n\n### VPC Endpoints (PrivateLink)\n\nTo avoid NAT Gateway costs and improve security, deploy VPC endpoints:\n\n- **com.amazonaws.us-east-1.kms** - KMS API calls\n- **com.amazonaws.us-east-1.secretsmanager** - Secrets retrieval\n- **com.amazonaws.us-east-1.logs** - CloudWatch Logs\n- **com.amazonaws.us-east-1.ecr.dkr** - ECR Docker registry\n- **com.amazonaws.us-east-1.ecr.api** - ECR API\n- **com.amazonaws.us-east-1.s3** (Gateway endpoint) - ECR layer storage\n\n## ECS Fargate Configuration\n\n### Cluster\n\n```hcl\nresource \"aws_ecs_cluster\" \"payment_token_service\" {\n  name = \"payment-token-service-${var.environment}\"\n\n  setting {\n    name  = \"containerInsights\"\n    value = \"enabled\"  # CloudWatch Container Insights\n  }\n\n  configuration {\n    execute_command_configuration {\n      logging = \"OVERRIDE\"\n      log_configuration {\n        cloud_watch_log_group_name = \"/ecs/payment-token-service\"\n      }\n    }\n  }\n}\n```\n\n### Task Definition\n\n```hcl\nresource \"aws_ecs_task_definition\" \"payment_token_service\" {\n  family                   = \"payment-token-service\"\n  network_mode             = \"awsvpc\"\n  requires_compatibilities = [\"FARGATE\"]\n  cpu                      = var.task_cpu       # 512, 1024, 2048, 4096\n  memory                   = var.task_memory    # 1024, 2048, 4096, 8192\n  execution_role_arn       = aws_iam_role.ecs_execution.arn\n  task_role_arn            = aws_iam_role.ecs_task.arn\n\n  container_definitions = jsonencode([{\n    name  = \"payment-token-service\"\n    image = \"${aws_ecr_repository.payment_token_service.repository_url}:${var.image_tag}\"\n    \n    portMappings = [\n      {\n        containerPort = 8000\n        protocol      = \"tcp\"\n        name          = \"public-api\"\n      },\n      {\n        containerPort = 8001\n        protocol      = \"tcp\"\n        name          = \"internal-api\"\n      }\n    ]\n\n    environment = [\n      { name = \"ENVIRONMENT\", value = var.environment },\n      { name = \"AWS_REGION\", value = var.aws_region },\n      { name = \"LOG_LEVEL\", value = var.log_level },\n      { name = \"PUBLIC_API_PORT\", value = \"8000\" },\n      { name = \"INTERNAL_API_PORT\", value = \"8001\" }\n    ]\n\n    secrets = [\n      {\n        name      = \"DATABASE_URL\"\n        valueFrom = \"${aws_secretsmanager_secret.pts_db_url.arn}\"\n      },\n      {\n        name      = \"BDK_KMS_KEY_ID\"\n        valueFrom = \"${aws_secretsmanager_secret.bdk_kms_key_id.arn}\"\n      },\n      {\n        name      = \"CURRENT_ENCRYPTION_KEY_VERSION\"\n        valueFrom = \"${aws_secretsmanager_secret.current_key_version.arn}\"\n      }\n    ]\n\n    logConfiguration = {\n      logDriver = \"awslogs\"\n      options = {\n        \"awslogs-group\"         = \"/ecs/payment-token-service\"\n        \"awslogs-region\"        = var.aws_region\n        \"awslogs-stream-prefix\" = \"ecs\"\n      }\n    }\n\n    healthCheck = {\n      command     = [\"CMD-SHELL\", \"curl -f http://localhost:8000/health || exit 1\"]\n      interval    = 30\n      timeout     = 5\n      retries     = 3\n      startPeriod = 60\n    }\n\n    # Security\n    readonlyRootFilesystem = false  # Poetry requires writable filesystem\n    user                   = \"1000:1000\"  # Non-root user\n\n    # Resource limits\n    ulimits = [{\n      name      = \"nofile\"\n      softLimit = 65536\n      hardLimit = 65536\n    }]\n  }])\n\n  # Ephemeral storage (for temp files during request processing)\n  ephemeral_storage {\n    size_in_gib = 21  # Default is 20, can go up to 200\n  }\n}\n```\n\n### ECS Service\n\n```hcl\nresource \"aws_ecs_service\" \"payment_token_service\" {\n  name            = \"payment-token-service\"\n  cluster         = aws_ecs_cluster.payment_token_service.id\n  task_definition = aws_ecs_task_definition.payment_token_service.arn\n  desired_count   = var.desired_count  # Start with 2, autoscale to 10+\n  launch_type     = \"FARGATE\"\n  platform_version = \"1.4.0\"  # Latest Fargate platform\n\n  network_configuration {\n    subnets         = aws_subnet.pci_zone[*].id  # PCI isolated subnets\n    security_groups = [aws_security_group.pts_ecs.id]\n    assign_public_ip = false  # Private subnets only\n  }\n\n  # Public API Load Balancer (via API Gateway)\n  load_balancer {\n    target_group_arn = aws_lb_target_group.pts_public.arn\n    container_name   = \"payment-token-service\"\n    container_port   = 8000\n  }\n\n  # Internal API Load Balancer (auth workers)\n  load_balancer {\n    target_group_arn = aws_lb_target_group.pts_internal.arn\n    container_name   = \"payment-token-service\"\n    container_port   = 8001\n  }\n\n  # Blue-Green Deployment\n  deployment_controller {\n    type = \"ECS\"  # Use ECS rolling deployment (can switch to CODE_DEPLOY for blue-green)\n  }\n\n  deployment_configuration {\n    maximum_percent         = 200  # Allow double capacity during deployment\n    minimum_healthy_percent = 100  # Always maintain full capacity\n    \n    deployment_circuit_breaker {\n      enable   = true\n      rollback = true  # Auto-rollback on failure\n    }\n  }\n\n  # Service discovery (optional, for internal service mesh)\n  service_registries {\n    registry_arn = aws_service_discovery_service.pts.arn\n  }\n\n  # Prevent service updates from recreating tasks unnecessarily\n  lifecycle {\n    ignore_changes = [desired_count]  # Autoscaling manages this\n  }\n\n  depends_on = [\n    aws_lb_listener.public_https,\n    aws_lb_listener.internal_https,\n    aws_iam_role_policy_attachment.ecs_execution,\n    aws_iam_role_policy_attachment.ecs_task\n  ]\n}\n```\n\n## Load Balancers\n\n### Public API - Application Load Balancer (ALB)\n\n```hcl\nresource \"aws_lb\" \"public_api\" {\n  name               = \"pts-public-api-${var.environment}\"\n  internal           = true  # Internal to VPC, accessed via API Gateway VPC Link\n  load_balancer_type = \"application\"\n  security_groups    = [aws_security_group.alb_public.id]\n  subnets            = aws_subnet.public[*].id\n\n  enable_deletion_protection = var.environment == \"production\"\n  enable_http2              = true\n  enable_cross_zone_load_balancing = true\n\n  access_logs {\n    bucket  = aws_s3_bucket.lb_logs.id\n    prefix  = \"public-api\"\n    enabled = true\n  }\n\n  tags = {\n    Name        = \"pts-public-api\"\n    Environment = var.environment\n    PCI         = \"true\"\n  }\n}\n\n# Target Group for public API (port 8000)\nresource \"aws_lb_target_group\" \"pts_public\" {\n  name        = \"pts-public-${var.environment}\"\n  port        = 8000\n  protocol    = \"HTTP\"\n  vpc_id      = aws_vpc.main.id\n  target_type = \"ip\"  # Required for Fargate\n\n  health_check {\n    enabled             = true\n    healthy_threshold   = 2\n    unhealthy_threshold = 3\n    timeout             = 5\n    interval            = 30\n    path                = \"/health\"\n    protocol            = \"HTTP\"\n    matcher             = \"200\"\n  }\n\n  deregistration_delay = 30  # Drain connections gracefully\n\n  stickiness {\n    type            = \"lb_cookie\"\n    enabled         = false  # Stateless service\n  }\n}\n\n# HTTPS Listener (API Gateway → ALB)\nresource \"aws_lb_listener\" \"public_https\" {\n  load_balancer_arn = aws_lb.public_api.arn\n  port              = \"443\"\n  protocol          = \"HTTPS\"\n  ssl_policy        = \"ELBSecurityPolicy-TLS13-1-2-2021-06\"  # TLS 1.3 only\n  certificate_arn   = aws_acm_certificate.pts_public.arn\n\n  default_action {\n    type             = \"forward\"\n    target_group_arn = aws_lb_target_group.pts_public.arn\n  }\n}\n```\n\n### Internal API - Network Load Balancer (NLB)\n\n```hcl\nresource \"aws_lb\" \"internal_api\" {\n  name               = \"pts-internal-api-${var.environment}\"\n  internal           = true  # VPC-only, for auth workers\n  load_balancer_type = \"network\"\n  subnets            = aws_subnet.pci_zone[*].id\n\n  enable_deletion_protection       = var.environment == \"production\"\n  enable_cross_zone_load_balancing = true\n\n  tags = {\n    Name        = \"pts-internal-api\"\n    Environment = var.environment\n    PCI         = \"true\"\n  }\n}\n\n# Target Group for internal API (port 8001, mTLS)\nresource \"aws_lb_target_group\" \"pts_internal\" {\n  name        = \"pts-internal-${var.environment}\"\n  port        = 8001\n  protocol    = \"TCP\"  # NLB uses TCP, mTLS handled at application layer\n  vpc_id      = aws_vpc.main.id\n  target_type = \"ip\"\n\n  health_check {\n    enabled             = true\n    healthy_threshold   = 2\n    unhealthy_threshold = 3\n    timeout             = 10\n    interval            = 30\n    protocol            = \"TCP\"\n    port                = \"8001\"\n  }\n\n  deregistration_delay = 30\n}\n\n# TCP Listener (mTLS enforced at application layer)\nresource \"aws_lb_listener\" \"internal_https\" {\n  load_balancer_arn = aws_lb.internal_api.arn\n  port              = \"8001\"\n  protocol          = \"TCP\"\n\n  default_action {\n    type             = \"forward\"\n    target_group_arn = aws_lb_target_group.pts_internal.arn\n  }\n}\n```\n\n## API Gateway\n\n### REST API Configuration\n\n```hcl\nresource \"aws_api_gateway_rest_api\" \"payment_token_service\" {\n  name        = \"payment-token-service-${var.environment}\"\n  description = \"Payment Token Service Public API\"\n\n  endpoint_configuration {\n    types = [\"REGIONAL\"]  # Use CloudFront for edge optimization if needed\n  }\n\n  binary_media_types = [\"application/x-protobuf\"]  # Protobuf support\n}\n\n# VPC Link (API Gateway → Internal ALB)\nresource \"aws_api_gateway_vpc_link\" \"pts\" {\n  name        = \"pts-vpc-link-${var.environment}\"\n  target_arns = [aws_lb.public_api.arn]\n}\n\n# Resource: /v1/payment-tokens\nresource \"aws_api_gateway_resource\" \"payment_tokens\" {\n  rest_api_id = aws_api_gateway_rest_api.payment_token_service.id\n  parent_id   = aws_api_gateway_rest_api.payment_token_service.root_resource_id\n  path_part   = \"v1\"\n}\n\nresource \"aws_api_gateway_resource\" \"payment_tokens_collection\" {\n  rest_api_id = aws_api_gateway_rest_api.payment_token_service.id\n  parent_id   = aws_api_gateway_resource.payment_tokens.id\n  path_part   = \"payment-tokens\"\n}\n\n# POST /v1/payment-tokens\nresource \"aws_api_gateway_method\" \"create_token\" {\n  rest_api_id   = aws_api_gateway_rest_api.payment_token_service.id\n  resource_id   = aws_api_gateway_resource.payment_tokens_collection.id\n  http_method   = \"POST\"\n  authorization = \"CUSTOM\"  # Custom authorizer for API key validation\n  authorizer_id = aws_api_gateway_authorizer.api_key.id\n\n  request_parameters = {\n    \"method.request.header.X-Idempotency-Key\" = true\n    \"method.request.header.Authorization\"     = true\n  }\n}\n\n# Integration with VPC Link\nresource \"aws_api_gateway_integration\" \"create_token\" {\n  rest_api_id = aws_api_gateway_rest_api.payment_token_service.id\n  resource_id = aws_api_gateway_resource.payment_tokens_collection.id\n  http_method = aws_api_gateway_method.create_token.http_method\n\n  type                    = \"HTTP_PROXY\"\n  integration_http_method = \"POST\"\n  uri                     = \"https://${aws_lb.public_api.dns_name}/v1/payment-tokens\"\n  connection_type         = \"VPC_LINK\"\n  connection_id           = aws_api_gateway_vpc_link.pts.id\n\n  request_parameters = {\n    \"integration.request.header.X-Idempotency-Key\" = \"method.request.header.X-Idempotency-Key\"\n    \"integration.request.header.Authorization\"     = \"method.request.header.Authorization\"\n  }\n\n  timeout_milliseconds = 29000  # API Gateway max is 29s\n}\n\n# Rate Limiting (Usage Plan)\nresource \"aws_api_gateway_usage_plan\" \"standard\" {\n  name = \"pts-standard-${var.environment}\"\n\n  throttle_settings {\n    burst_limit = 2000   # Burst capacity\n    rate_limit  = 1000   # Sustained requests per second\n  }\n\n  quota_settings {\n    limit  = 1000000  # 1M requests per month\n    period = \"MONTH\"\n  }\n\n  api_stages {\n    api_id = aws_api_gateway_rest_api.payment_token_service.id\n    stage  = aws_api_gateway_stage.production.stage_name\n  }\n}\n\n# WAF for API Gateway\nresource \"aws_wafv2_web_acl\" \"api_gateway\" {\n  name  = \"pts-api-gateway-${var.environment}\"\n  scope = \"REGIONAL\"\n\n  default_action {\n    allow {}\n  }\n\n  # Rate limiting rule (per IP)\n  rule {\n    name     = \"rate-limit-per-ip\"\n    priority = 1\n\n    action {\n      block {}\n    }\n\n    statement {\n      rate_based_statement {\n        limit              = 2000  # Requests per 5 minutes\n        aggregate_key_type = \"IP\"\n      }\n    }\n\n    visibility_config {\n      cloudwatch_metrics_enabled = true\n      metric_name                = \"RateLimitPerIP\"\n      sampled_requests_enabled   = true\n    }\n  }\n\n  # AWS Managed Rules - Core rule set\n  rule {\n    name     = \"aws-managed-core-rules\"\n    priority = 2\n\n    override_action {\n      none {}\n    }\n\n    statement {\n      managed_rule_group_statement {\n        vendor_name = \"AWS\"\n        name        = \"AWSManagedRulesCommonRuleSet\"\n      }\n    }\n\n    visibility_config {\n      cloudwatch_metrics_enabled = true\n      metric_name                = \"AWSManagedCoreRules\"\n      sampled_requests_enabled   = true\n    }\n  }\n\n  visibility_config {\n    cloudwatch_metrics_enabled = true\n    metric_name                = \"PTSAPIGatewayWAF\"\n    sampled_requests_enabled   = true\n  }\n}\n```\n\n## RDS Database (Isolated)\n\n### PostgreSQL Instance Configuration\n\n```hcl\nresource \"aws_db_instance\" \"payment_tokens_db\" {\n  identifier     = \"payment-tokens-db-${var.environment}\"\n  engine         = \"postgres\"\n  engine_version = \"15.4\"  # Latest PostgreSQL 15\n  instance_class = var.db_instance_class  # db.t3.medium for dev, db.r6g.xlarge for prod\n\n  allocated_storage     = 100  # GB, can autoscale\n  max_allocated_storage = 1000 # Autoscale up to 1TB\n  storage_type          = \"gp3\"\n  storage_encrypted     = true\n  kms_key_id            = aws_kms_key.rds_encryption.arn\n\n  db_name  = \"payment_tokens_db\"\n  username = \"pts_admin\"\n  password = random_password.db_password.result  # Managed by Terraform, stored in Secrets Manager\n\n  # Network\n  db_subnet_group_name   = aws_db_subnet_group.payment_tokens.name\n  vpc_security_group_ids = [aws_security_group.rds_pts.id]\n  publicly_accessible    = false\n\n  # High Availability\n  multi_az = var.environment == \"production\"\n\n  # Backups\n  backup_retention_period = var.environment == \"production\" ? 30 : 7  # 30 days for prod\n  backup_window           = \"03:00-04:00\"  # UTC\n  maintenance_window      = \"mon:04:00-mon:05:00\"  # UTC\n\n  # Performance\n  performance_insights_enabled    = true\n  performance_insights_kms_key_id = aws_kms_key.performance_insights.arn\n  performance_insights_retention_period = 7\n\n  enabled_cloudwatch_logs_exports = [\"postgresql\", \"upgrade\"]\n\n  # Deletion protection\n  deletion_protection = var.environment == \"production\"\n  skip_final_snapshot = var.environment != \"production\"\n  final_snapshot_identifier = var.environment == \"production\" ? \"payment-tokens-db-final-${formatdate(\"YYYY-MM-DD-hhmm\", timestamp())}\" : null\n\n  # Parameter group for PCI compliance\n  parameter_group_name = aws_db_parameter_group.payment_tokens.name\n\n  tags = {\n    Name        = \"payment-tokens-db\"\n    Environment = var.environment\n    PCI         = \"true\"\n    Backup      = \"required\"\n  }\n}\n\n# DB Subnet Group (data tier subnets)\nresource \"aws_db_subnet_group\" \"payment_tokens\" {\n  name       = \"payment-tokens-${var.environment}\"\n  subnet_ids = aws_subnet.data_tier[*].id\n\n  tags = {\n    Name = \"payment-tokens-db-subnet-group\"\n  }\n}\n\n# Parameter Group (security hardening)\nresource \"aws_db_parameter_group\" \"payment_tokens\" {\n  name   = \"payment-tokens-pg15-${var.environment}\"\n  family = \"postgres15\"\n\n  # Enforce SSL connections\n  parameter {\n    name  = \"rds.force_ssl\"\n    value = \"1\"\n  }\n\n  # Audit logging for PCI compliance\n  parameter {\n    name  = \"log_connections\"\n    value = \"1\"\n  }\n\n  parameter {\n    name  = \"log_disconnections\"\n    value = \"1\"\n  }\n\n  parameter {\n    name  = \"log_statement\"\n    value = \"all\"  # Log all SQL statements (mod or ddl for less verbose)\n  }\n\n  parameter {\n    name  = \"log_min_duration_statement\"\n    value = \"1000\"  # Log queries slower than 1s\n  }\n\n  # Connection pooling\n  parameter {\n    name  = \"max_connections\"\n    value = var.db_max_connections  # 100 for dev, 500 for prod\n  }\n}\n\n# Read Replica (optional, for scaling reads)\nresource \"aws_db_instance\" \"payment_tokens_replica\" {\n  count = var.environment == \"production\" ? 1 : 0\n\n  identifier     = \"payment-tokens-db-replica-${var.environment}\"\n  replicate_source_db = aws_db_instance.payment_tokens_db.identifier\n\n  instance_class = var.db_instance_class\n  publicly_accessible = false\n\n  # Inherit most settings from primary\n  \n  tags = {\n    Name        = \"payment-tokens-db-replica\"\n    Environment = var.environment\n    PCI         = \"true\"\n  }\n}\n```\n\n## KMS Key Management\n\n### Base Derivation Key (BDK)\n\n```hcl\nresource \"aws_kms_key\" \"bdk\" {\n  description             = \"Payment Token Service - Base Derivation Key (BDK)\"\n  deletion_window_in_days = 30\n  enable_key_rotation     = true  # Automatic annual rotation\n\n  # Key policy (most restrictive)\n  policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Sid    = \"Enable IAM User Permissions\"\n        Effect = \"Allow\"\n        Principal = {\n          AWS = \"arn:aws:iam::${data.aws_caller_identity.current.account_id}:root\"\n        }\n        Action   = \"kms:*\"\n        Resource = \"*\"\n      },\n      {\n        Sid    = \"Allow Payment Token Service to Decrypt BDK\"\n        Effect = \"Allow\"\n        Principal = {\n          AWS = aws_iam_role.ecs_task.arn\n        }\n        Action = [\n          \"kms:Decrypt\",\n          \"kms:DescribeKey\"\n        ]\n        Resource = \"*\"\n        Condition = {\n          StringEquals = {\n            \"kms:EncryptionContext:service\" = \"payment-token-service\"\n          }\n        }\n      }\n    ]\n  })\n\n  tags = {\n    Name        = \"pts-bdk\"\n    Environment = var.environment\n    PCI         = \"true\"\n    Rotation    = \"annual\"\n  }\n}\n\nresource \"aws_kms_alias\" \"bdk\" {\n  name          = \"alias/pts-bdk-${var.environment}\"\n  target_key_id = aws_kms_key.bdk.key_id\n}\n```\n\n### Service Encryption Keys (Rotating)\n\n```hcl\nresource \"aws_kms_key\" \"service_encryption\" {\n  description             = \"Payment Token Service - Service Encryption Key (Rotating)\"\n  deletion_window_in_days = 30\n  enable_key_rotation     = true  # Rotate every 90 days (managed via Lambda)\n\n  policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Sid    = \"Enable IAM User Permissions\"\n        Effect = \"Allow\"\n        Principal = {\n          AWS = \"arn:aws:iam::${data.aws_caller_identity.current.account_id}:root\"\n        }\n        Action   = \"kms:*\"\n        Resource = \"*\"\n      },\n      {\n        Sid    = \"Allow Payment Token Service Encryption/Decryption\"\n        Effect = \"Allow\"\n        Principal = {\n          AWS = aws_iam_role.ecs_task.arn\n        }\n        Action = [\n          \"kms:Encrypt\",\n          \"kms:Decrypt\",\n          \"kms:GenerateDataKey\",\n          \"kms:DescribeKey\"\n        ]\n        Resource = \"*\"\n      }\n    ]\n  })\n\n  tags = {\n    Name        = \"pts-service-encryption\"\n    Environment = var.environment\n    PCI         = \"true\"\n  }\n}\n```\n\n### RDS Encryption Key\n\n```hcl\nresource \"aws_kms_key\" \"rds_encryption\" {\n  description             = \"Payment Tokens RDS Database Encryption\"\n  deletion_window_in_days = 30\n  enable_key_rotation     = true\n\n  tags = {\n    Name        = \"pts-rds-encryption\"\n    Environment = var.environment\n    PCI         = \"true\"\n  }\n}\n```\n\n## Autoscaling\n\n### Target Tracking - CPU and Memory\n\n```hcl\nresource \"aws_appautoscaling_target\" \"ecs_target\" {\n  max_capacity       = var.max_capacity  # 20 for prod, 5 for dev\n  min_capacity       = var.min_capacity  # 2 for prod, 1 for dev\n  resource_id        = \"service/${aws_ecs_cluster.payment_token_service.name}/${aws_ecs_service.payment_token_service.name}\"\n  scalable_dimension = \"ecs:service:DesiredCount\"\n  service_namespace  = \"ecs\"\n}\n\n# Scale on CPU utilization\nresource \"aws_appautoscaling_policy\" \"ecs_cpu\" {\n  name               = \"cpu-autoscaling\"\n  policy_type        = \"TargetTrackingScaling\"\n  resource_id        = aws_appautoscaling_target.ecs_target.resource_id\n  scalable_dimension = aws_appautoscaling_target.ecs_target.scalable_dimension\n  service_namespace  = aws_appautoscaling_target.ecs_target.service_namespace\n\n  target_tracking_scaling_policy_configuration {\n    target_value       = 70.0  # Target 70% CPU\n    scale_in_cooldown  = 300   # 5 minutes before scaling in\n    scale_out_cooldown = 60    # 1 minute before scaling out\n\n    predefined_metric_specification {\n      predefined_metric_type = \"ECSServiceAverageCPUUtilization\"\n    }\n  }\n}\n\n# Scale on Memory utilization\nresource \"aws_appautoscaling_policy\" \"ecs_memory\" {\n  name               = \"memory-autoscaling\"\n  policy_type        = \"TargetTrackingScaling\"\n  resource_id        = aws_appautoscaling_target.ecs_target.resource_id\n  scalable_dimension = aws_appautoscaling_target.ecs_target.scalable_dimension\n  service_namespace  = aws_appautoscaling_target.ecs_target.service_namespace\n\n  target_tracking_scaling_policy_configuration {\n    target_value       = 80.0  # Target 80% memory\n    scale_in_cooldown  = 300\n    scale_out_cooldown = 60\n\n    predefined_metric_specification {\n      predefined_metric_type = \"ECSServiceAverageMemoryUtilization\"\n    }\n  }\n}\n```\n\n### Step Scaling - Request Count\n\n```hcl\nresource \"aws_appautoscaling_policy\" \"ecs_request_count\" {\n  name               = \"request-count-step-scaling\"\n  policy_type        = \"StepScaling\"\n  resource_id        = aws_appautoscaling_target.ecs_target.resource_id\n  scalable_dimension = aws_appautoscaling_target.ecs_target.scalable_dimension\n  service_namespace  = aws_appautoscaling_target.ecs_target.service_namespace\n\n  step_scaling_policy_configuration {\n    adjustment_type         = \"PercentChangeInCapacity\"\n    cooldown                = 60\n    metric_aggregation_type = \"Average\"\n\n    # Scale out aggressively\n    step_adjustment {\n      metric_interval_lower_bound = 0\n      metric_interval_upper_bound = 10\n      scaling_adjustment          = 10  # Add 10% capacity\n    }\n\n    step_adjustment {\n      metric_interval_lower_bound = 10\n      metric_interval_upper_bound = 20\n      scaling_adjustment          = 20  # Add 20% capacity\n    }\n\n    step_adjustment {\n      metric_interval_lower_bound = 20\n      scaling_adjustment          = 30  # Add 30% capacity\n    }\n  }\n}\n\n# CloudWatch Alarm for request count scaling\nresource \"aws_cloudwatch_metric_alarm\" \"request_count_high\" {\n  alarm_name          = \"pts-request-count-high-${var.environment}\"\n  comparison_operator = \"GreaterThanThreshold\"\n  evaluation_periods  = 2\n  metric_name         = \"RequestCountPerTarget\"\n  namespace           = \"AWS/ApplicationELB\"\n  period              = 60\n  statistic           = \"Sum\"\n  threshold           = 1000  # 1000 requests per target per minute\n  alarm_description   = \"Triggers when request count is too high\"\n  alarm_actions       = [aws_appautoscaling_policy.ecs_request_count.arn]\n\n  dimensions = {\n    TargetGroup  = aws_lb_target_group.pts_public.arn_suffix\n    LoadBalancer = aws_lb.public_api.arn_suffix\n  }\n}\n```\n\n### Scheduled Scaling (Optional)\n\n```hcl\nresource \"aws_appautoscaling_scheduled_action\" \"scale_up_morning\" {\n  count = var.enable_scheduled_scaling ? 1 : 0\n\n  name               = \"scale-up-morning\"\n  service_namespace  = aws_appautoscaling_target.ecs_target.service_namespace\n  resource_id        = aws_appautoscaling_target.ecs_target.resource_id\n  scalable_dimension = aws_appautoscaling_target.ecs_target.scalable_dimension\n  schedule           = \"cron(0 6 * * ? *)\"  # 6 AM UTC daily\n\n  scalable_target_action {\n    min_capacity = var.min_capacity * 2\n    max_capacity = var.max_capacity\n  }\n}\n```\n\n## IAM Roles and Policies\n\n### ECS Task Execution Role (For ECS to pull images, write logs)\n\n```hcl\nresource \"aws_iam_role\" \"ecs_execution\" {\n  name = \"pts-ecs-execution-${var.environment}\"\n\n  assume_role_policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [{\n      Action = \"sts:AssumeRole\"\n      Effect = \"Allow\"\n      Principal = {\n        Service = \"ecs-tasks.amazonaws.com\"\n      }\n    }]\n  })\n}\n\nresource \"aws_iam_role_policy_attachment\" \"ecs_execution\" {\n  role       = aws_iam_role.ecs_execution.name\n  policy_arn = \"arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy\"\n}\n\n# Additional policy for Secrets Manager\nresource \"aws_iam_role_policy\" \"ecs_execution_secrets\" {\n  name = \"secrets-access\"\n  role = aws_iam_role.ecs_execution.id\n\n  policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Effect = \"Allow\"\n        Action = [\n          \"secretsmanager:GetSecretValue\"\n        ]\n        Resource = [\n          aws_secretsmanager_secret.pts_db_url.arn,\n          aws_secretsmanager_secret.bdk_kms_key_id.arn,\n          aws_secretsmanager_secret.current_key_version.arn\n        ]\n      },\n      {\n        Effect = \"Allow\"\n        Action = [\n          \"kms:Decrypt\"\n        ]\n        Resource = [\n          aws_kms_key.secrets_encryption.arn\n        ]\n      }\n    ]\n  })\n}\n```\n\n### ECS Task Role (For application to access AWS services)\n\n```hcl\nresource \"aws_iam_role\" \"ecs_task\" {\n  name = \"pts-ecs-task-${var.environment}\"\n\n  assume_role_policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [{\n      Action = \"sts:AssumeRole\"\n      Effect = \"Allow\"\n      Principal = {\n        Service = \"ecs-tasks.amazonaws.com\"\n      }\n    }]\n  })\n}\n\n# KMS access for BDK decryption\nresource \"aws_iam_role_policy\" \"ecs_task_kms\" {\n  name = \"kms-access\"\n  role = aws_iam_role.ecs_task.id\n\n  policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Sid    = \"DecryptBDK\"\n        Effect = \"Allow\"\n        Action = [\n          \"kms:Decrypt\",\n          \"kms:DescribeKey\"\n        ]\n        Resource = aws_kms_key.bdk.arn\n        Condition = {\n          StringEquals = {\n            \"kms:EncryptionContext:service\" = \"payment-token-service\"\n          }\n        }\n      },\n      {\n        Sid    = \"ServiceEncryption\"\n        Effect = \"Allow\"\n        Action = [\n          \"kms:Encrypt\",\n          \"kms:Decrypt\",\n          \"kms:GenerateDataKey\",\n          \"kms:DescribeKey\"\n        ]\n        Resource = aws_kms_key.service_encryption.arn\n      }\n    ]\n  })\n}\n\n# CloudWatch Logs\nresource \"aws_iam_role_policy\" \"ecs_task_logs\" {\n  name = \"cloudwatch-logs\"\n  role = aws_iam_role.ecs_task.id\n\n  policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [{\n      Effect = \"Allow\"\n      Action = [\n        \"logs:CreateLogStream\",\n        \"logs:PutLogEvents\"\n      ]\n      Resource = \"${aws_cloudwatch_log_group.ecs.arn}:*\"\n    }]\n  })\n}\n```\n\n## Monitoring and Observability\n\n### CloudWatch Log Groups\n\n```hcl\nresource \"aws_cloudwatch_log_group\" \"ecs\" {\n  name              = \"/ecs/payment-token-service\"\n  retention_in_days = var.environment == \"production\" ? 2555 : 30  # 7 years for PCI compliance\n\n  kms_key_id = aws_kms_key.cloudwatch_logs.arn\n\n  tags = {\n    Name        = \"pts-ecs-logs\"\n    Environment = var.environment\n    PCI         = \"true\"\n  }\n}\n\n# VPC Flow Logs (PCI compliance)\nresource \"aws_flow_log\" \"pci_zone\" {\n  iam_role_arn    = aws_iam_role.flow_logs.arn\n  log_destination = aws_cloudwatch_log_group.vpc_flow_logs.arn\n  traffic_type    = \"ALL\"\n  vpc_id          = aws_vpc.main.id\n\n  tags = {\n    Name = \"pts-pci-zone-flow-logs\"\n    PCI  = \"true\"\n  }\n}\n\nresource \"aws_cloudwatch_log_group\" \"vpc_flow_logs\" {\n  name              = \"/vpc/payment-token-service\"\n  retention_in_days = var.environment == \"production\" ? 2555 : 90\n\n  tags = {\n    PCI = \"true\"\n  }\n}\n```\n\n### CloudWatch Alarms\n\n```hcl\n# High error rate alarm\nresource \"aws_cloudwatch_metric_alarm\" \"high_error_rate\" {\n  alarm_name          = \"pts-high-error-rate-${var.environment}\"\n  comparison_operator = \"GreaterThanThreshold\"\n  evaluation_periods  = 2\n  metric_name         = \"HTTPCode_Target_5XX_Count\"\n  namespace           = \"AWS/ApplicationELB\"\n  period              = 300\n  statistic           = \"Sum\"\n  threshold           = 10\n  alarm_description   = \"Alert when 5xx errors exceed threshold\"\n  alarm_actions       = [aws_sns_topic.alerts.arn]\n\n  dimensions = {\n    LoadBalancer = aws_lb.public_api.arn_suffix\n    TargetGroup  = aws_lb_target_group.pts_public.arn_suffix\n  }\n}\n\n# Database connection failures\nresource \"aws_cloudwatch_metric_alarm\" \"db_connection_failures\" {\n  alarm_name          = \"pts-db-connection-failures-${var.environment}\"\n  comparison_operator = \"GreaterThanThreshold\"\n  evaluation_periods  = 1\n  metric_name         = \"DatabaseConnectionFailures\"\n  namespace           = \"AWS/RDS\"\n  period              = 60\n  statistic           = \"Sum\"\n  threshold           = 5\n  alarm_description   = \"Alert on database connection failures\"\n  alarm_actions       = [aws_sns_topic.alerts.arn]\n\n  dimensions = {\n    DBInstanceIdentifier = aws_db_instance.payment_tokens_db.id\n  }\n}\n\n# KMS throttling\nresource \"aws_cloudwatch_metric_alarm\" \"kms_throttle\" {\n  alarm_name          = \"pts-kms-throttling-${var.environment}\"\n  comparison_operator = \"GreaterThanThreshold\"\n  evaluation_periods  = 2\n  metric_name         = \"UserErrorCount\"\n  namespace           = \"AWS/KMS\"\n  period              = 60\n  statistic           = \"Sum\"\n  threshold           = 10\n  alarm_description   = \"Alert on KMS throttling\"\n  alarm_actions       = [aws_sns_topic.alerts.arn]\n\n  dimensions = {\n    KeyId = aws_kms_key.bdk.id\n  }\n}\n\n# Unauthorized decrypt attempts (security)\nresource \"aws_cloudwatch_log_metric_filter\" \"unauthorized_decrypt\" {\n  name           = \"unauthorized-decrypt-attempts\"\n  log_group_name = aws_cloudwatch_log_group.ecs.name\n  pattern        = \"[timestamp, request_id, level=ERROR, event=unauthorized_decrypt_attempt, ...]\"\n\n  metric_transformation {\n    name      = \"UnauthorizedDecryptAttempts\"\n    namespace = \"PaymentTokenService\"\n    value     = \"1\"\n  }\n}\n\nresource \"aws_cloudwatch_metric_alarm\" \"unauthorized_decrypt_alarm\" {\n  alarm_name          = \"pts-unauthorized-decrypt-${var.environment}\"\n  comparison_operator = \"GreaterThanThreshold\"\n  evaluation_periods  = 1\n  metric_name         = \"UnauthorizedDecryptAttempts\"\n  namespace           = \"PaymentTokenService\"\n  period              = 300\n  statistic           = \"Sum\"\n  threshold           = 10\n  treat_missing_data  = \"notBreaching\"\n  alarm_description   = \"Security alert: Unauthorized decrypt attempts\"\n  alarm_actions       = [aws_sns_topic.security_alerts.arn]\n}\n```\n\n### CloudWatch Dashboard\n\n```hcl\nresource \"aws_cloudwatch_dashboard\" \"payment_token_service\" {\n  dashboard_name = \"payment-token-service-${var.environment}\"\n\n  dashboard_body = jsonencode({\n    widgets = [\n      {\n        type = \"metric\"\n        properties = {\n          metrics = [\n            [\"AWS/ECS\", \"CPUUtilization\", { stat = \"Average\" }],\n            [\".\", \"MemoryUtilization\", { stat = \"Average\" }]\n          ]\n          period = 300\n          stat   = \"Average\"\n          region = var.aws_region\n          title  = \"ECS Resource Utilization\"\n        }\n      },\n      {\n        type = \"metric\"\n        properties = {\n          metrics = [\n            [\"AWS/ApplicationELB\", \"TargetResponseTime\", { stat = \"p99\" }],\n            [\"...\", { stat = \"p95\" }],\n            [\"...\", { stat = \"p50\" }]\n          ]\n          period = 60\n          stat   = \"Average\"\n          region = var.aws_region\n          title  = \"API Response Time (Latency)\"\n        }\n      },\n      {\n        type = \"metric\"\n        properties = {\n          metrics = [\n            [\"AWS/ApplicationELB\", \"HTTPCode_Target_2XX_Count\", { stat = \"Sum\" }],\n            [\".\", \"HTTPCode_Target_4XX_Count\", { stat = \"Sum\" }],\n            [\".\", \"HTTPCode_Target_5XX_Count\", { stat = \"Sum\" }]\n          ]\n          period = 300\n          stat   = \"Sum\"\n          region = var.aws_region\n          title  = \"HTTP Response Codes\"\n        }\n      },\n      {\n        type = \"metric\"\n        properties = {\n          metrics = [\n            [\"AWS/RDS\", \"DatabaseConnections\", { stat = \"Average\" }],\n            [\".\", \"CPUUtilization\", { stat = \"Average\" }],\n            [\".\", \"FreeableMemory\", { stat = \"Average\" }]\n          ]\n          period = 300\n          stat   = \"Average\"\n          region = var.aws_region\n          title  = \"RDS Performance\"\n        }\n      }\n    ]\n  })\n}\n```\n\n## Secrets Management\n\n```hcl\n# Database URL\nresource \"aws_secretsmanager_secret\" \"pts_db_url\" {\n  name                    = \"pts/database-url-${var.environment}\"\n  recovery_window_in_days = 30\n  kms_key_id              = aws_kms_key.secrets_encryption.arn\n\n  tags = {\n    Environment = var.environment\n    PCI         = \"true\"\n  }\n}\n\nresource \"aws_secretsmanager_secret_version\" \"pts_db_url\" {\n  secret_id = aws_secretsmanager_secret.pts_db_url.id\n  secret_string = \"postgresql://${aws_db_instance.payment_tokens_db.username}:${random_password.db_password.result}@${aws_db_instance.payment_tokens_db.endpoint}/${aws_db_instance.payment_tokens_db.db_name}\"\n}\n\n# BDK KMS Key ID\nresource \"aws_secretsmanager_secret\" \"bdk_kms_key_id\" {\n  name                    = \"pts/bdk-kms-key-id-${var.environment}\"\n  recovery_window_in_days = 30\n  kms_key_id              = aws_kms_key.secrets_encryption.arn\n}\n\nresource \"aws_secretsmanager_secret_version\" \"bdk_kms_key_id\" {\n  secret_id     = aws_secretsmanager_secret.bdk_kms_key_id.id\n  secret_string = aws_kms_key.bdk.arn\n}\n\n# Current encryption key version\nresource \"aws_secretsmanager_secret\" \"current_key_version\" {\n  name                    = \"pts/current-encryption-key-version-${var.environment}\"\n  recovery_window_in_days = 7\n  kms_key_id              = aws_kms_key.secrets_encryption.arn\n}\n\nresource \"aws_secretsmanager_secret_version\" \"current_key_version\" {\n  secret_id     = aws_secretsmanager_secret.current_key_version.id\n  secret_string = jsonencode({\n    version = \"v1\"\n    key_id  = aws_kms_key.service_encryption.id\n  })\n}\n```\n\n## Deployment Strategy\n\n### Blue-Green Deployment with CodeDeploy\n\n```hcl\n# CodeDeploy Application\nresource \"aws_codedeploy_app\" \"payment_token_service\" {\n  name             = \"payment-token-service-${var.environment}\"\n  compute_platform = \"ECS\"\n}\n\n# Deployment Group\nresource \"aws_codedeploy_deployment_group\" \"payment_token_service\" {\n  app_name               = aws_codedeploy_app.payment_token_service.name\n  deployment_group_name  = \"pts-${var.environment}\"\n  service_role_arn       = aws_iam_role.codedeploy.arn\n  deployment_config_name = \"CodeDeployDefault.ECSAllAtOnce\"  # or Custom for canary\n\n  auto_rollback_configuration {\n    enabled = true\n    events  = [\"DEPLOYMENT_FAILURE\", \"DEPLOYMENT_STOP_ON_ALARM\"]\n  }\n\n  blue_green_deployment_config {\n    terminate_blue_instances_on_deployment_success {\n      action                           = \"TERMINATE\"\n      termination_wait_time_in_minutes = 5\n    }\n\n    deployment_ready_option {\n      action_on_timeout = \"CONTINUE_DEPLOYMENT\"\n    }\n  }\n\n  deployment_style {\n    deployment_option = \"WITH_TRAFFIC_CONTROL\"\n    deployment_type   = \"BLUE_GREEN\"\n  }\n\n  ecs_service {\n    cluster_name = aws_ecs_cluster.payment_token_service.name\n    service_name = aws_ecs_service.payment_token_service.name\n  }\n\n  load_balancer_info {\n    target_group_pair_info {\n      prod_traffic_route {\n        listener_arns = [aws_lb_listener.public_https.arn]\n      }\n\n      target_group {\n        name = aws_lb_target_group.pts_public.name\n      }\n\n      target_group {\n        name = aws_lb_target_group.pts_public_blue.name  # Blue target group\n      }\n    }\n  }\n\n  alarm_configuration {\n    alarms  = [aws_cloudwatch_metric_alarm.high_error_rate.alarm_name]\n    enabled = true\n  }\n}\n```\n\n## Terraform Module Structure\n\n```\nterraform/\n├── environments/\n│   ├── dev/\n│   │   ├── main.tf\n│   │   ├── variables.tf\n│   │   └── terraform.tfvars\n│   ├── staging/\n│   │   ├── main.tf\n│   │   ├── variables.tf\n│   │   └── terraform.tfvars\n│   └── production/\n│       ├── main.tf\n│       ├── variables.tf\n│       └── terraform.tfvars\n│\n├── modules/\n│   ├── networking/\n│   │   ├── main.tf          # VPC, subnets, security groups, VPC endpoints\n│   │   ├── variables.tf\n│   │   └── outputs.tf\n│   │\n│   ├── ecs/\n│   │   ├── main.tf          # ECS cluster, task definition, service\n│   │   ├── variables.tf\n│   │   └── outputs.tf\n│   │\n│   ├── load-balancer/\n│   │   ├── main.tf          # ALB, NLB, target groups, listeners\n│   │   ├── variables.tf\n│   │   └── outputs.tf\n│   │\n│   ├── rds/\n│   │   ├── main.tf          # RDS instance, parameter group, subnet group\n│   │   ├── variables.tf\n│   │   └── outputs.tf\n│   │\n│   ├── kms/\n│   │   ├── main.tf          # KMS keys (BDK, service encryption, RDS)\n│   │   ├── variables.tf\n│   │   └── outputs.tf\n│   │\n│   ├── api-gateway/\n│   │   ├── main.tf          # API Gateway, VPC Link, WAF\n│   │   ├── variables.tf\n│   │   └── outputs.tf\n│   │\n│   ├── monitoring/\n│   │   ├── main.tf          # CloudWatch alarms, dashboards, log groups\n│   │   ├── variables.tf\n│   │   └── outputs.tf\n│   │\n│   └── autoscaling/\n│       ├── main.tf          # Autoscaling policies, scheduled actions\n│       ├── variables.tf\n│       └── outputs.tf\n│\n└── backend.tf               # S3 backend for Terraform state\n```\n\n### Example: environments/production/main.tf\n\n```hcl\nterraform {\n  required_version = \">= 1.5.0\"\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"\n    }\n  }\n\n  backend \"s3\" {\n    bucket         = \"payments-infra-terraform-state\"\n    key            = \"production/payment-token-service/terraform.tfstate\"\n    region         = \"us-east-1\"\n    encrypt        = true\n    kms_key_id     = \"arn:aws:kms:us-east-1:ACCOUNT_ID:key/TERRAFORM_STATE_KEY_ID\"\n    dynamodb_table = \"terraform-state-lock\"\n  }\n}\n\nprovider \"aws\" {\n  region = var.aws_region\n\n  default_tags {\n    tags = {\n      Environment = \"production\"\n      Project     = \"payments-infra\"\n      Service     = \"payment-token-service\"\n      ManagedBy   = \"terraform\"\n      PCI         = \"true\"\n    }\n  }\n}\n\nmodule \"networking\" {\n  source = \"../../modules/networking\"\n\n  environment          = \"production\"\n  vpc_cidr             = \"10.0.0.0/16\"\n  availability_zones   = [\"us-east-1a\", \"us-east-1b\"]\n  enable_vpc_endpoints = true\n}\n\nmodule \"kms\" {\n  source = \"../../modules/kms\"\n\n  environment = \"production\"\n}\n\nmodule \"rds\" {\n  source = \"../../modules/rds\"\n\n  environment       = \"production\"\n  instance_class    = \"db.r6g.xlarge\"\n  allocated_storage = 500\n  multi_az          = true\n  \n  vpc_id            = module.networking.vpc_id\n  subnet_ids        = module.networking.data_tier_subnet_ids\n  security_group_id = module.networking.rds_security_group_id\n  kms_key_arn       = module.kms.rds_encryption_key_arn\n}\n\nmodule \"ecs\" {\n  source = \"../../modules/ecs\"\n\n  environment = \"production\"\n  \n  cluster_name  = \"payment-token-service\"\n  image_tag     = var.image_tag\n  task_cpu      = 2048\n  task_memory   = 4096\n  desired_count = 4\n\n  vpc_id               = module.networking.vpc_id\n  subnet_ids           = module.networking.pci_zone_subnet_ids\n  security_group_id    = module.networking.ecs_security_group_id\n  \n  bdk_kms_key_arn      = module.kms.bdk_key_arn\n  service_kms_key_arn  = module.kms.service_encryption_key_arn\n  database_secret_arn  = module.rds.db_secret_arn\n}\n\nmodule \"load_balancer\" {\n  source = \"../../modules/load-balancer\"\n\n  environment = \"production\"\n  \n  vpc_id             = module.networking.vpc_id\n  public_subnet_ids  = module.networking.public_subnet_ids\n  pci_subnet_ids     = module.networking.pci_zone_subnet_ids\n  \n  alb_security_group_id = module.networking.alb_security_group_id\n  ecs_security_group_id = module.networking.ecs_security_group_id\n  \n  certificate_arn = var.acm_certificate_arn\n}\n\nmodule \"api_gateway\" {\n  source = \"../../modules/api-gateway\"\n\n  environment = \"production\"\n  \n  alb_arn         = module.load_balancer.public_alb_arn\n  enable_waf      = true\n  rate_limit      = 2000\n}\n\nmodule \"autoscaling\" {\n  source = \"../../modules/autoscaling\"\n\n  environment = \"production\"\n  \n  ecs_cluster_name = module.ecs.cluster_name\n  ecs_service_name = module.ecs.service_name\n  \n  min_capacity = 4\n  max_capacity = 20\n  \n  cpu_target    = 70\n  memory_target = 80\n}\n\nmodule \"monitoring\" {\n  source = \"../../modules/monitoring\"\n\n  environment = \"production\"\n  \n  alb_arn            = module.load_balancer.public_alb_arn\n  target_group_arn   = module.load_balancer.public_target_group_arn\n  ecs_cluster_name   = module.ecs.cluster_name\n  ecs_service_name   = module.ecs.service_name\n  rds_instance_id    = module.rds.instance_id\n  \n  alert_email = var.alert_email\n}\n\noutput \"api_gateway_url\" {\n  value = module.api_gateway.api_url\n}\n\noutput \"internal_nlb_dns\" {\n  value = module.load_balancer.internal_nlb_dns\n}\n```\n\n### Example: environments/production/terraform.tfvars\n\n```hcl\naws_region = \"us-east-1\"\nenvironment = \"production\"\n\n# ECS Configuration\nimage_tag = \"v1.2.3\"  # Updated by CI/CD pipeline\ntask_cpu = 2048       # 2 vCPU\ntask_memory = 4096    # 4 GB\n\n# Autoscaling\nmin_capacity = 4\nmax_capacity = 20\n\n# Database\ndb_instance_class = \"db.r6g.xlarge\"\ndb_allocated_storage = 500\ndb_max_connections = 500\n\n# Alerts\nalert_email = \"oncall-payments@example.com\"\n\n# TLS Certificate\nacm_certificate_arn = \"arn:aws:acm:us-east-1:ACCOUNT_ID:certificate/CERT_ID\"\n```\n\n## CI/CD Pipeline Integration\n\n### GitHub Actions Example\n\n```yaml\nname: Deploy Payment Token Service\n\non:\n  push:\n    branches: [main]\n    paths:\n      - 'services/payment-token/**'\n      - 'terraform/modules/**'\n      - 'terraform/environments/production/**'\n\nenv:\n  AWS_REGION: us-east-1\n  ECR_REPOSITORY: payment-token-service\n  ECS_SERVICE: payment-token-service\n  ECS_CLUSTER: payment-token-service-production\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      \n      - name: Run unit tests\n        run: |\n          cd services/payment-token\n          poetry install\n          poetry run pytest tests/unit\n      \n      - name: Run integration tests\n        run: |\n          cd services/payment-token\n          poetry run pytest tests/integration\n      \n      - name: Run security scan\n        uses: aquasecurity/trivy-action@master\n        with:\n          scan-type: 'fs'\n          scan-ref: 'services/payment-token'\n\n  build:\n    needs: test\n    runs-on: ubuntu-latest\n    outputs:\n      image-tag: ${{ steps.build-image.outputs.image }}\n    steps:\n      - uses: actions/checkout@v3\n      \n      - name: Configure AWS credentials\n        uses: aws-actions/configure-aws-credentials@v2\n        with:\n          role-to-assume: ${{ secrets.AWS_DEPLOY_ROLE_ARN }}\n          aws-region: ${{ env.AWS_REGION }}\n      \n      - name: Login to Amazon ECR\n        id: login-ecr\n        uses: aws-actions/amazon-ecr-login@v1\n      \n      - name: Build and push Docker image\n        id: build-image\n        env:\n          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}\n          IMAGE_TAG: ${{ github.sha }}\n        run: |\n          docker build -f services/payment-token/Dockerfile -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG .\n          docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG\n          echo \"image=$ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG\" >> $GITHUB_OUTPUT\n\n  terraform-plan:\n    needs: build\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      \n      - name: Setup Terraform\n        uses: hashicorp/setup-terraform@v2\n        with:\n          terraform_version: 1.5.0\n      \n      - name: Terraform Init\n        run: |\n          cd terraform/environments/production\n          terraform init\n      \n      - name: Terraform Plan\n        run: |\n          cd terraform/environments/production\n          terraform plan -var=\"image_tag=${{ needs.build.outputs.image-tag }}\" -out=tfplan\n      \n      - name: Upload plan\n        uses: actions/upload-artifact@v3\n        with:\n          name: tfplan\n          path: terraform/environments/production/tfplan\n\n  deploy:\n    needs: [build, terraform-plan]\n    runs-on: ubuntu-latest\n    environment: production\n    steps:\n      - uses: actions/checkout@v3\n      \n      - name: Setup Terraform\n        uses: hashicorp/setup-terraform@v2\n      \n      - name: Download plan\n        uses: actions/download-artifact@v3\n        with:\n          name: tfplan\n          path: terraform/environments/production\n      \n      - name: Terraform Apply\n        run: |\n          cd terraform/environments/production\n          terraform apply -auto-approve tfplan\n      \n      - name: Run smoke tests\n        run: |\n          # Wait for deployment\n          sleep 60\n          \n          # Test health endpoint\n          API_URL=$(cd terraform/environments/production && terraform output -raw api_gateway_url)\n          curl -f $API_URL/health || exit 1\n```\n\n## Cost Optimization\n\n### Resource Sizing Recommendations\n\n**Development Environment:**\n- ECS Tasks: 512 CPU / 1024 Memory, 1-2 tasks\n- RDS: db.t3.medium, Single-AZ\n- NAT Gateway: 1 gateway\n- Estimated cost: ~$200-300/month\n\n**Production Environment:**\n- ECS Tasks: 2048 CPU / 4096 Memory, 4-20 tasks (autoscale)\n- RDS: db.r6g.xlarge, Multi-AZ with read replica\n- NAT Gateway: 2 gateways (Multi-AZ)\n- VPC Endpoints: 6 endpoints (~$7/endpoint/month)\n- Estimated cost: ~$1,500-3,000/month (depending on load)\n\n### Cost Saving Strategies\n\n1. **Use VPC Endpoints** instead of NAT Gateway for AWS services ($0.01/GB vs $0.045/GB)\n2. **Fargate Spot** for non-production environments (70% cost reduction)\n3. **Reserved Capacity** for RDS in production (40% savings)\n4. **S3 Lifecycle Policies** for audit log archival (move to Glacier after 90 days)\n5. **CloudWatch Log Retention** policies (retain only what's needed for PCI)\n\n## Security Hardening Checklist\n\n- [ ] All connections use TLS 1.3\n- [ ] mTLS enforced for internal `/decrypt` endpoint\n- [ ] BDK never leaves KMS (use KMS Decrypt API)\n- [ ] Database encrypted at rest with KMS\n- [ ] VPC Flow Logs enabled for PCI zone\n- [ ] CloudTrail logging enabled for all KMS operations\n- [ ] IAM roles follow principle of least privilege\n- [ ] Security groups deny all by default\n- [ ] No public IPs assigned to ECS tasks\n- [ ] Secrets stored in Secrets Manager, not environment variables\n- [ ] Container runs as non-root user\n- [ ] Read-only root filesystem (if possible)\n- [ ] WAF rules enabled on API Gateway\n- [ ] Rate limiting configured per restaurant\n- [ ] Audit logs retained for 7 years (PCI compliance)\n- [ ] Regular security scanning (Trivy, Snyk, etc.)\n\n## Disaster Recovery\n\n### Backup Strategy\n\n**RDS Automated Backups:**\n- Daily snapshots, 30-day retention\n- Automated backup window: 03:00-04:00 UTC\n- Cross-region backup replication for production\n\n**Manual Snapshots:**\n- Before major deployments\n- Before database migrations\n- Retained indefinitely or until explicitly deleted\n\n**Point-in-Time Recovery (PITR):**\n- RDS supports PITR within backup retention period\n- RPO: 5 minutes (transaction log backups)\n\n### Recovery Procedures\n\n**RTO (Recovery Time Objective): 1 hour**\n**RPO (Recovery Point Objective): 5 minutes**\n\n**Scenario 1: Complete AZ Failure**\n- Multi-AZ RDS automatically fails over (1-2 minutes)\n- ECS tasks redistribute across remaining AZ (5 minutes)\n- Update Route 53 health checks if needed\n\n**Scenario 2: Regional Failure (DR)**\n- Restore RDS from cross-region snapshot\n- Deploy ECS tasks in DR region using Terraform\n- Update API Gateway to point to DR region\n- Estimated time: 30-60 minutes\n\n**Scenario 3: Database Corruption**\n- Restore from latest automated snapshot\n- Apply transaction logs for PITR\n- Validate data integrity\n- Estimated time: 15-30 minutes\n\n## Runbooks\n\n### Key Rotation - Service Encryption Keys (Every 90 Days)\n\n1. Generate new KMS key version via Terraform:\n   ```bash\n   terraform apply -var=\"rotate_service_key=true\"\n   ```\n\n2. Update `encryption_keys` table via migration:\n   ```sql\n   INSERT INTO encryption_keys (key_version, kms_key_id, is_active)\n   VALUES ('v2', 'arn:aws:kms:...', true);\n   \n   UPDATE encryption_keys SET is_active = false WHERE key_version = 'v1';\n   ```\n\n3. Deploy updated service configuration (new key version)\n\n4. Background job re-encrypts old tokens over 30 days\n\n5. After 90 days, schedule KMS key deletion\n\n### BDK Rotation (Annual)\n\n1. Create new BDK in KMS via Terraform\n2. Update Secrets Manager with new BDK ARN\n3. Deploy service with support for both BDKs\n4. New tokens use new BDK immediately\n5. After 24 hours (token TTL), retire old BDK\n\n### Zero-Downtime Deployment\n\n1. CI/CD builds new Docker image, pushes to ECR\n2. Terraform updates ECS task definition with new image\n3. ECS performs rolling deployment:\n   - Starts new tasks (green)\n   - Waits for health checks to pass\n   - Drains connections from old tasks (blue)\n   - Terminates old tasks\n4. Circuit breaker auto-rolls back on failure\n\n## Production Readiness Checklist\n\n- [ ] All Terraform modules tested in staging\n- [ ] Database migration tested with production-like data\n- [ ] Load testing completed (target: 500 RPS)\n- [ ] Disaster recovery procedures tested\n- [ ] Runbooks documented and validated\n- [ ] Monitoring dashboards configured\n- [ ] Alerting thresholds tuned\n- [ ] PCI compliance audit passed\n- [ ] Security penetration testing completed\n- [ ] On-call rotation established\n- [ ] Incident response procedures documented\n- [ ] API documentation published\n- [ ] SLAs defined and agreed upon\n\n## Dependencies\n\n- **Blocked by**: \n  - Payment Token Service application code complete [[s-7ujm]]\n  - Docker image builds successfully\n  - E2E tests passing [[i-24o2]]\n  \n- **Depends on**:\n  - AWS account with appropriate permissions\n  - Domain name and DNS hosted zone (Route 53)\n  - TLS certificates (ACM)\n  - Terraform state backend (S3 + DynamoDB)\n\n## Success Criteria\n\n- [ ] Infrastructure deployed via Terraform with no manual steps\n- [ ] All resources tagged appropriately\n- [ ] ECS service autoscales based on load\n- [ ] Blue-green deployments working\n- [ ] All monitoring alarms functional\n- [ ] PCI compliance requirements met\n- [ ] Load testing passes at 500+ RPS\n- [ ] Disaster recovery tested and validated\n- [ ] Documentation complete","priority":0,"archived":0,"archived_at":null,"created_at":"2025-11-11 06:43:32","updated_at":"2025-11-11 06:43:32","parent_id":null,"parent_uuid":null,"relationships":[{"from":"s-5for","from_type":"spec","to":"i-3a1m","to_type":"issue","type":"implements"},{"from":"s-5for","from_type":"spec","to":"s-7ujm","to_type":"spec","type":"implements"},{"from":"s-5for","from_type":"spec","to":"s-8c0t","to_type":"spec","type":"references"}],"tags":["aws","deployment","infrastructure","pci-compliant","terraform"]}
{"id":"s-1wow","uuid":"a147b6e4-c7ee-472a-bd8b-e3b9b2499eda","title":"CI/CD Pipeline, Git Hooks, and Test Infrastructure for Monorepo","file_path":"specs/ci_cd_pipeline_git_hooks_and_test_infrastructure_f.md","content":"## Overview\n\nImplement a comprehensive testing and CI/CD infrastructure for the payments-infra monorepo to ensure code quality, prevent regressions, and provide fast feedback loops for developers.\n\n## Goals\n\n1. **Fast feedback loops**: Developers should get test results within seconds locally, minutes in CI\n2. **Prevent bad commits**: Strict pre-commit hooks that block commits on any failure\n3. **Intelligent test execution**: Run only tests affected by changes, but always run critical integration/e2e tests\n4. **Parallel execution**: Run tests for different services in parallel to maximize speed\n5. **Developer ergonomics**: Easy to use, clear error messages, minimal friction\n\n## Architecture\n\n### Monorepo Structure\n```\npayments-infra/\n├── services/\n│   ├── payment-token/           (unit + integration tests)\n│   ├── authorization-api/       (unit + integration tests)\n│   └── auth-processor-worker/   (unit + integration tests)\n├── shared/\n│   ├── payments_common/         (unit tests)\n│   └── payments_proto/          (no tests, generated code)\n├── tests/                       (e2e tests using tox)\n└── infrastructure/\n    └── migrations/              (no tests)\n```\n\n### Change Detection Strategy\n\n**To be determined** - See [[i-TBD]] for investigation of approaches:\n- Custom Python script (git diff based)\n- Nx/Turborepo (sophisticated caching)\n- GitHub Actions path filters\n\nRequirements:\n- Detect which services changed based on file paths\n- Detect when shared packages change (affects ALL services)\n- Support local execution (git hooks) and CI execution\n- Output list of affected services to run tests for\n\n### Test Execution Strategy\n\n#### Test Levels\n1. **Unit tests**: Fast, no external dependencies, per-service\n2. **Integration tests**: Database, SQS, external APIs, per-service\n3. **E2E tests**: Full system tests across all services (always run)\n\n#### Test Caching Rules\n- **Unit tests**: Cache results based on service code + dependencies hash\n- **Integration tests**: Always run for affected services (no caching)\n- **E2E tests**: Always run (critical path, no caching)\n\n#### Execution Priority\n```\nLocal (git hooks):\n  pre-commit: format → lint → typecheck → unit tests (staged files)\n  pre-push: integration tests (affected services)\n\nCI (GitHub Actions):\n  PR: unit → integration → e2e (parallel service jobs)\n  Main: unit → integration → e2e → (future: auto-deploy)\n```\n\n## Components\n\n### 1. Git Hooks (Pre-commit Framework)\n\n**File**: `.pre-commit-config.yaml`\n\nHooks to implement:\n- **black**: Auto-format Python code (all staged files)\n- **ruff**: Lint and fix issues (all staged files)\n- **mypy**: Type check (affected services only)\n- **pytest (unit)**: Run unit tests for affected services\n- **proto validation**: Ensure protobuf files are valid\n\n**Strictness**: Block commits if ANY check fails\n\nConfiguration:\n- Run in parallel where possible (black + ruff can run together)\n- Skip certain hooks with `SKIP=hook-name` env var for emergencies\n- Clear error messages pointing to the failing file and fix instructions\n\n### 2. Claude Code Hooks\n\n**File**: `.claude/hooks.yaml` (or similar)\n\nHooks:\n- **on-file-save**: Auto-format file, show lint warnings\n- **on-file-change**: Run unit tests for affected service (background)\n- **on-git-commit-attempt**: Surface pre-commit hook results in Claude interface\n- **on-pr-create**: Show CI status and test results\n\nBenefits:\n- Developers get instant feedback without leaving Claude\n- Test failures surface immediately during development\n- Reduces context switching\n\n### 3. GitHub Actions CI/CD Pipeline\n\n**Files**: \n- `.github/workflows/ci.yml` (main CI pipeline)\n- `.github/workflows/pr.yml` (PR-specific checks)\n\n#### Main CI Pipeline (`ci.yml`)\n\nTrigger: `push` to any branch, `pull_request`\n\nJobs:\n```yaml\n1. setup:\n   - Checkout code\n   - Setup Python 3.11\n   - Cache poetry dependencies (per service)\n   - Detect changed services (using change detection tool)\n   \n2. lint-and-typecheck:\n   - Run ruff on all services\n   - Run mypy on all services\n   - Fast feedback on code quality\n   \n3. test-[service-name] (matrix job):\n   - For each affected service in parallel:\n     - Run unit tests\n     - Run integration tests\n     - Upload coverage reports\n   \n4. test-e2e:\n   - Start docker-compose (localstack, postgres, etc.)\n   - Run tox e2e tests\n   - Always run (critical path)\n   \n5. report:\n   - Aggregate coverage from all jobs\n   - Comment on PR with results\n   - Fail if coverage drops below threshold\n```\n\n#### Parallel Execution Strategy\n- Lint/typecheck runs once for all code\n- Each service gets its own test job (runs in parallel)\n- E2E tests run after all service tests pass\n- Maximum parallelism with dependencies\n\n#### Caching Strategy\n```yaml\n- Poetry dependencies: Hash pyproject.toml + poetry.lock per service\n- Pytest cache: Hash test files + source files\n- Docker layers: Cache intermediate build stages\n- Tox environments: Cache .tox directory (invalidate on deps change)\n```\n\n### 4. Change Detection Tool\n\n**File**: `scripts/detect_changes.py` (or similar)\n\nInput: `base_ref` (e.g., `origin/main`), `head_ref` (current commit)\n\nOutput: JSON with affected services\n```json\n{\n  \"services\": [\"payment-token\", \"auth-processor-worker\"],\n  \"shared_changed\": true,\n  \"run_all_tests\": false\n}\n```\n\nLogic:\n- If `shared/` changes: run tests for ALL services\n- If `services/X/` changes: run tests for service X\n- If `tests/` changes: run e2e tests\n- If `infrastructure/migrations/` changes: run integration tests for all services\n\n### 5. Test Orchestration Scripts\n\n**Files**: \n- `scripts/run_affected_tests.sh`\n- `scripts/run_tests_parallel.sh`\n\nUsage:\n```bash\n# Local development\n./scripts/run_affected_tests.sh --level=unit --base=main\n\n# CI\n./scripts/run_tests_parallel.sh --services=\"payment-token,authorization-api\"\n```\n\nFeatures:\n- Colorized output\n- Progress indicators\n- Time tracking per service\n- Fail fast or collect all failures\n- Summary report at end\n\n## Success Criteria\n\n1. **Pre-commit hooks**:\n   - Block commits with failing tests\n   - Run in < 30 seconds for typical changes\n   - Clear error messages\n\n2. **CI Pipeline**:\n   - PR feedback in < 5 minutes for service-only changes\n   - Full pipeline (all services + e2e) in < 15 minutes\n   - Parallel service jobs save 50%+ time vs sequential\n\n3. **Developer Experience**:\n   - One command setup: `make setup-hooks`\n   - Hooks can be bypassed in emergencies: `git commit --no-verify`\n   - CI results visible in PR comments\n\n4. **Test Coverage**:\n   - Maintain > 80% coverage for all services\n   - No commits that decrease coverage without justification\n\n## Implementation Phases\n\n### Phase 1: Git Hooks (Highest Priority)\n- Set up pre-commit framework\n- Configure black, ruff, mypy hooks\n- Add unit test execution for affected services\n- Document how to install and use\n\n### Phase 2: Change Detection\n- Investigate approaches (Nx, custom script, path filters)\n- Implement chosen solution\n- Test with various change scenarios\n\n### Phase 3: GitHub Actions CI\n- Create ci.yml workflow\n- Implement parallel service test jobs\n- Add e2e test job\n- Configure caching\n\n### Phase 4: Test Orchestration\n- Create helper scripts for running tests\n- Integrate change detection with test execution\n- Add progress reporting\n\n### Phase 5: Claude Code Integration\n- Configure Claude hooks (if supported)\n- Integrate with pre-commit results\n- Add test running capabilities\n\n### Phase 6: Optimization\n- Fine-tune caching strategies\n- Optimize test selection logic\n- Reduce CI run times\n- Add test result caching\n\n## Open Questions\n\n1. Should we use pre-commit framework or custom git hooks?\n2. What's the right balance between speed and thoroughness in pre-commit?\n3. Should integration tests run in pre-commit or only pre-push?\n4. Do we want branch protection rules in GitHub?\n5. Should we add automatic dependency updates (Dependabot)?\n\n## Related Issues\n\n- [[i-TBD]]: Investigate monorepo change detection approaches\n","priority":0,"archived":0,"archived_at":null,"created_at":"2025-11-13 09:47:07","updated_at":"2025-11-13 09:47:07","parent_id":null,"parent_uuid":null,"relationships":[],"tags":["ci-cd","infrastructure","monorepo","testing"]}
